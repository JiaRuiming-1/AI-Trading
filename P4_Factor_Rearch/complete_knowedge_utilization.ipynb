{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Data From Tushare\n",
    "We load data from 2018-2021. Due to some constraint of platform, we download thses data year by year and save seperately. After that we process these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(ts.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# register token\n",
    "token = '' # your token\n",
    "ts.set_token(token)\n",
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# got calendar from date range\n",
    "start_date = '20180101'\n",
    "end_date = '20210101'\n",
    "calendar = pro.trade_cal(exchange='SSE', is_open='1', \n",
    "                            start_date=start_date, \n",
    "                            end_date=end_date, \n",
    "                            fields='cal_date')\n",
    "# check all stocks exist in market today\n",
    "stocks = pro.query('stock_basic', exchange='', list_status='L', market = '主板') # 主板/创业板/科创板/CDR/北交所\n",
    "ts_code_list = ','.join(stocks.ts_code.values)\n",
    "print(calendar.shape, stocks.shape)\n",
    "calendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get fundamental data\n",
    "# pick up market cap [5,30] billion\n",
    "base_universe = pro.bak_daily(trade_date='20180102', \n",
    "                   fields='trade_date, ts_code, name, float_mv, total_mv, pe, turn_over, industry')\n",
    "filte_stock = base_universe.loc[(base_universe.total_mv>=50) & (base_universe.total_mv<=300)]\n",
    "filte_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from helper import download_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load total stock daily date for one year\n",
    "ts_code_list = filte_stock.ts_code.values\n",
    "all_stocks = download_helper.get_Daily_All(ts, ts_code_list, start_date, end_date)\n",
    "print(all_stocks.shape)\n",
    "all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "universe = all_stocks.drop_duplicates()\n",
    "universe.to_csv(start_date +'-'+ end_date + '.csv')\n",
    "filte_stock.drop_duplicates().to_csv('fundamental_' + start_date +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Data by File\n",
    "if we load data from saved file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "universe = pd.read_csv('20180101-20210101.csv').iloc[:,1:]\n",
    "fundamental = pd.read_csv('fundamental_20180101.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Process Data\n",
    "1. filter ma_v_120 top 500 stocks\n",
    "2. add 'date' column as datetime type, and deascanding time\n",
    "3. add industry infomation and boll indicator to stock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.factor_helper import IndicatorHelper\n",
    "\n",
    "ind_helper = IndicatorHelper(universe)\n",
    "\n",
    "# pick average amount 120 days top 500\n",
    "universe = ind_helper.top(500, index='trade_date', ticker_column='ts_code', value_column='ma_v_120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add bollinger as indicator which will be used as a custom factor later\n",
    "# the bollinger indicator make up by stockstats package which depends on column nameed close as default\n",
    "tech_indicator_list = ['boll_ub','boll_lb']\n",
    "universe = ind_helper.add_technical_indicator(tech_indicator_list)\n",
    "\n",
    "# add industry and stock name\n",
    "universe = ind_helper.add_by_basetable('ts_code', fundamental, ['industry', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Factors\n",
    "### Overnight Returns and Firm-Specific Investor Sentiment\n",
    "the overnight return calculate by $\\frac{open_t - close_{t-1}}{close_{t-1}}$\n",
    " \n",
    " paper calculate price by sum average 5 days as long factor, we just average 5 days\n",
    " \n",
    " use average 20 days of overnight return as a short factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.factor_helper import CloseToOpen\n",
    "\n",
    "# cal close to open average moving 5day as long facor and 20day as short factor\n",
    "cto = CloseToOpen(universe).calculate()\n",
    "universe = cto.get_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Winners and Losers in Momentum Investing\n",
    " The stock price tragectories can be expressed by $p=\\mu*time + \\beta*time^2$ \n",
    " \n",
    " We convert time as linner values and get $\\mu$ and $\\beta$ by regression method between price and constant values. \n",
    " \n",
    " Final facotor expressed $\\beta * \\mu$\n",
    " \n",
    " This factor can express each stock tragectories relative convex. The $\\mu$ be viewed as return direction and $\\beta$ be viwed as return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression use `statsmodels.formula.api` package\n",
    "from helper.factor_helper import WinnerAndLoser\n",
    "wl = WinnerAndLoser(universe, win_length=20).calculate()\n",
    "universe = wl.get_factor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Expected Skewness and Momentum\n",
    "The skewness of returns distribution and media in a period time(20 trade day) can combine to be a factor.\n",
    " \n",
    "factor = $skew * median$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.factor_helper import SkewandMomentum\n",
    "sm = SkewandMomentum(universe).calculate()\n",
    "universe = sm.get_factor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(universe.shape)\n",
    "universe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrage Asymmetry and the Idiosyncratic Volatility Puzzle\n",
    "Based on the last parper, we use idiosyncratic martix and bollinger indicator to construct custom factors. \n",
    "\n",
    "### PCA risk model\n",
    "we use pct_chg column to calculate covariance matrix $F=\\frac{1}{N-1}rr^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20230303 start tmp load\n",
    "# load data from csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "universe = pd.read_csv('tmp_result.csv').iloc[:,1:]\n",
    "universe.date = pd.to_datetime(universe.date)\n",
    "print(universe.shape)\n",
    "universe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [10.0, 6.0]\n",
    "\n",
    "class RiskModel(object):\n",
    "    def __init__(self, returns, ann_factor, num_factor_exposures):\n",
    "        \n",
    "        self.num_factor_exposures = num_factor_exposures\n",
    "        self.pca = PCA(n_components=num_factor_exposures, svd_solver=svd_solver)\n",
    "        self.pca.fit(returns)\n",
    "        \n",
    "        self.factor_betas_ = self.factor_betas(self.pca, returns.columns.values, np.arange(num_factor_exposures))\n",
    "        self.factor_returns_ = self.factor_returns(self.pca, returns, returns.index, np.arange(num_factor_exposures))\n",
    "        self.factor_cov_matrix_ = self.factor_cov_matrix(self.factor_returns_, ann_factor)\n",
    "        \n",
    "        self.idiosyncratic_var_matrix_ = self.idiosyncratic_var_matrix(returns, \n",
    "                                            self.factor_returns_, self.factor_betas_, ann_factor)\n",
    "        self.idiosyncratic_var_vector = pd.DataFrame(data=np.diag(self.idiosyncratic_var_matrix_),\n",
    "                                                     index=returns.columns)\n",
    "    \n",
    "    # got new exposure expressed by pca model\n",
    "    def factor_betas(self, pca, factor_beta_indices, factor_beta_columns):\n",
    "        return pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\n",
    "    \n",
    "    # got new factor returns expressed by pca model\n",
    "    def factor_returns(self, pca, returns, factor_return_indices, factor_return_columns):\n",
    "        return pd.DataFrame(pca.transform(returns), factor_return_indices, factor_return_columns)\n",
    "    \n",
    "    # got new factor covariance matirx by pca expressed returns\n",
    "    def factor_cov_matrix(self, factor_returns, ann_factor):\n",
    "        return np.diag(factor_returns.var(axis=0, ddof=1) * ann_factor)\n",
    "    \n",
    "    # calculate idiosyncratic need to got factor_returns, factor_betas which calculate by pca model first\n",
    "    def idiosyncratic_var_matrix(self, returns, factor_returns, factor_betas, ann_factor):\n",
    "        estimate_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n",
    "        residuals = returns - estimate_returns\n",
    "        return pd.DataFrame(np.diag(np.var(residuals))*ann_factor, returns.columns, returns.columns)\n",
    "    \n",
    "    def plot_principle_risk(self):\n",
    "        # Make the bar plot\n",
    "        plt.bar(np.arange(self.num_factor_exposures), self.pca.explained_variance_ratio_);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got pivot dataframe index=time, columns=ticker values=pct_chg \n",
    "returns_df = universe.pivot(index='date', columns='ts_code', values='pct_chg').fillna(0)\n",
    "\n",
    "# Set the annualized factor\n",
    "ann_factor = 252\n",
    "\n",
    "# Set the number of factor exposures (principal components) for the PCA algorithm\n",
    "num_factor_exposures = 30\n",
    "\n",
    "# Set the svd solver for the PCA algorithm\n",
    "svd_solver = 'full'\n",
    "\n",
    "# Create a RiskModel object\n",
    "rm = RiskModel(returns_df, ann_factor, num_factor_exposures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view portfolio variance and idiosyncratic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = rm.factor_betas_\n",
    "F = rm.factor_cov_matrix_\n",
    "S = rm.idiosyncratic_var_matrix_\n",
    "# temperaory set all equal weights\n",
    "universe_tickers = universe.ts_code.unique()\n",
    "X = pd.DataFrame(np.repeat(1/len(universe_tickers), len(universe_tickers)), universe_tickers)\n",
    "\n",
    "variance = np.dot(X.T, (np.dot(B, F).dot(B.T) + S)).dot(X)\n",
    "variance = np.sqrt(variance[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'portfolio variance is: {variance}')\n",
    "print(rm.idiosyncratic_var_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rm.idiosyncratic_var_vector.loc[rm.idiosyncratic_var_vector.index=='603128.SH'])\n",
    "universe[['date','ts_code','boll_ub','boll_lb','close','vol','amount','ma_v_10']].loc[universe.ts_code == '603128.SH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Bollinger Factor\n",
    "As a simple view, I guess each stock residual value imply a magnitude of excess return.\n",
    "\n",
    "Note that, the residuals what we have calculated cross all the time. Indeed, we can't use it as a factor like that. Actually, we can't use any data as a factor which over pass the time we use to pridict return. For example, if we make up a factor in time T to predict T+1 return. we can't make up this factor by infomation which we archive from T+1 or further time.\n",
    "\n",
    "But, I use it cross all the time just verify my hypotheses.\n",
    "\n",
    "factor = (boll_ub + boll_lb - 2 * close) * residuals / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.factor_helper import BollingerAndResidual\n",
    "br = BollingerAndResidual(universe, rm.idiosyncratic_var_vector).calculate()\n",
    "universe = br.get_factor()\n",
    "universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute Factor\n",
    "Now, we can evalute these factors performence\n",
    "### rank factor and zscore\n",
    "First we group factors by industry, then rank and zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate facors and turn to zscore\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def rank_zscore(universe):\n",
    "    factor_columns = ['close_to_open_5_sma', 'close_to_open_25_sma', 'win_lose', 'skew_momentum', 'custom_factor']\n",
    "    all_factor_df = pd.DataFrame()\n",
    "    # itera all industry\n",
    "    for df_tuple in tqdm(universe.groupby('industry'), desc='industrt/industries'):\n",
    "        df_group = df_tuple[1]\n",
    "        factor_df = df_group[['date', 'ts_code']]\n",
    "        # intera all names of factors in raw table\n",
    "        for factor_name in factor_columns:\n",
    "            tmp = df_group.pivot(index='date', columns='ts_code', values=factor_name).fillna(0)\n",
    "            tmp = tmp.rank(axis=1).apply(zscore, axis=1)\n",
    "            X = tmp.stack().reset_index()\n",
    "            X.columns = ['date', 'ts_code', factor_name]\n",
    "            factor_df = factor_df.merge(X[[\"ts_code\", \"date\", factor_name]], on=[\"ts_code\", \"date\"], how=\"left\")\n",
    "        all_factor_df = all_factor_df.append(factor_df)\n",
    "\n",
    "    all_factor_df = all_factor_df.set_index(['date', 'ts_code'])\n",
    "    all_factor_df = all_factor_df.sort_values(by=[\"date\", \"ts_code\"])\n",
    "\n",
    "    return all_factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all factors table to multi index table that fit to use in alphalens\n",
    "all_factor_df = rank_zscore(universe)\n",
    "all_factor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process price\n",
    "Process price table in order to fit using by alphalens. Index=date, columns=ts_code\n",
    "\n",
    "Note: We're evaluating the alpha factors using delay of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = universe.pivot(index='date', columns='ts_code', values='close')\n",
    "prices = prices.shift(-1).iloc[:-1,:].fillna(method='bfill')\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data by alphalens\n",
    "Format alpha factors and pricing for Alphalens\n",
    "In order to use a lot of the alphalens functions, we need to aligned the indices and convert the time to unix timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "clean_factor_data = {\n",
    "    column: al.utils.get_clean_factor_and_forward_returns(factor=all_factor_df[column],\n",
    "                                                          prices=prices, periods=[1,5,20]).replace([np.inf, -np.inf], 0)\n",
    "    for column in all_factor_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['close_to_open_5_sma', 'close_to_open_25_sma', 'win_lose', 'skew_momentum', 'custom_factor']\n",
    "clean_factor_data['skew_momentum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Factor Returns\n",
    "Let's view the factor returns over time. We should be seeing it generally move up and to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_factor_returns_1day = pd.DataFrame()\n",
    "ls_factor_returns_5day = pd.DataFrame()\n",
    "ls_factor_returns_20day = pd.DataFrame()\n",
    "for factor, factor_data in clean_factor_data.items():\n",
    "    al_factor_returns = al.performance.factor_returns(factor_data)\n",
    "    ls_factor_returns_1day[factor] = al_factor_returns.iloc[:, 0]\n",
    "    ls_factor_returns_5day[factor] = al_factor_returns.iloc[:, 1]\n",
    "    ls_factor_returns_20day[factor] = al_factor_returns.iloc[:, 2]\n",
    "\n",
    "(1+ls_factor_returns_1day).cumprod().plot(title='quantile 1D')\n",
    "(1+ls_factor_returns_5day).cumprod().plot(title='quantile 5D')\n",
    "(1+ls_factor_returns_20day).cumprod().plot(title='quantile 20D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good alpha is also monotonic in quantiles. Let's looks the basis points for the factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in clean_factor_data.items():\n",
    "    qr_factor_returns[factor] = al.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 1]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(4,2),\n",
    "    figsize=(12, 8),\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpe Ratio of the Alphas\n",
    "Implement sharpe_ratio to calculate the sharpe ratio of factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(factor_returns):\n",
    "    \"\"\"\n",
    "    Get the sharpe ratio for each factor for the entire period\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns for each factor and date\n",
    "    annualization_factor: float\n",
    "        Annualization Factor\n",
    "    Returns\n",
    "    -------\n",
    "    sharpe_ratio : Pandas Series of floats\n",
    "        Sharpe ratio\n",
    "    \"\"\"\n",
    "    annualization_factor = np.sqrt(252)\n",
    "    return factor_returns.mean()/factor_returns.std()*annualization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio(ls_factor_returns_1day).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sharpe_ratio(ls_factor_returns_5day).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sharpe_ratio(ls_factor_returns_20day).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Analysis\n",
    "Let's look at analysis of my custom factor and overnight sma 25 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(clean_factor_data['custom_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(clean_factor_data['close_to_open_1_sma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all alpha factors\n",
    "all_factor_df.to_csv('factors_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load factors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_factor_df = pd.read_csv('factors_2020.csv')\n",
    "all_factor_df.date = pd.to_datetime(all_factor_df.date)\n",
    "all_factor_df = all_factor_df.set_index(['date', 'asset'])\n",
    "all_factor_df = all_factor_df.sort_values(by=[\"date\", \"asset\"])\n",
    "all_factor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Portfolio\n",
    "\n",
    "###  Combined Alpha Vector\n",
    "To use these alphas in a portfolio, we need to combine them somehow so we get a single score per stock. \n",
    "This is a area where machine learning can be very helpful. \n",
    "In this module, however, we will take the simplest approach of combination: \n",
    "\n",
    "\\['close_to_open_5_sma', 'close_to_open_25_sma', 'win_lose', 'skew_momentum', 'custom_factor'\\].\n",
    "\n",
    "\\[0.4, 0.3, 0.1, 0, 0.2\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_factors = all_factor_df.columns[[0, 1, 2, 3, 4]]\n",
    "print('Selected Factors: {}'.format(', '.join(selected_factors)))\n",
    "all_factor_df['alpha_vector'] = all_factor_df[selected_factors].mean(axis=1)\n",
    "# all_factor_df['alpha_vector'] = all_factor_df[selected_factors].dot(np.array([0.4,0.3,0.1,0.2]).T)\n",
    "alphas = all_factor_df[['alpha_vector']]\n",
    "alpha_vector = alphas.loc[all_factor_df.index.get_level_values(0)[-1]]\n",
    "alpha_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimization nomal method\n",
    "So far we have an alpha model and a risk model. \n",
    "I need to optimize portfolio that trades as close as possible to the alpha model but limiting risk as measured by the risk model. We also use covex optimization method.\n",
    "\n",
    "constraint contains risk cap, factor exposure, weight sum and weight range\n",
    "\n",
    "minize destination is - alpha * weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.optimizer_helper import OptimalHoldings\n",
    "'''\n",
    "View Data\n",
    "With the OptimalHoldings class implemented, let's see the weights it generates.\n",
    "It put most of the weight in a few stocks.\n",
    "'''\n",
    "optimal_weights = OptimalHoldings().find(alpha_vector, rm.factor_betas_,\n",
    "                                         rm.factor_cov_matrix_, rm.idiosyncratic_var_vector)\n",
    "print(max(optimal_weights.values), min(optimal_weights.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)\n",
    "risk_betas = rm.factor_betas_.loc[optimal_weights.index].T.dot(optimal_weights)\n",
    "#risk_betas.plot.bar(title='Portfolio Net Factor Exposures',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
