{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "In this notebook, we will complete backtest which is the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from statistics import median\n",
    "from scipy.stats import gaussian_kde, zscore\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Factors\n",
    "We have factors store in csv file which process and combine from privious steps. Pick up backtest time from 2022.4 - 2022.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fundamental_df = pd.read_csv('tmp_factor.csv').iloc[:,1:]\n",
    "universe_raw = pd.read_csv('AI_all.csv').iloc[:,1:]\n",
    "universe_raw['date'] = pd.to_datetime(universe_raw['trade_date'], format='%Y%m%d')\n",
    "universe = universe_raw.set_index(['date']).sort_values(by=['date'])\n",
    "universe = universe.fillna(method='ffill').fillna(0.)\n",
    "# only use from 2022.4 to 2023.3, here from 20220201 in order to generate risk model from history\n",
    "universe = universe.loc[universe['trade_date']>=20221001]\n",
    "universe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zscore AI factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix excuteed in privious step\n",
    "base_field = ['ts_code','trade_date','name', 'industry', 'close', 'log-ret']\n",
    "alpha_field = ['alpha_atr', 'alpha_010', 'alpha_149', 'alpha_AI'] \n",
    "\n",
    "universe[alpha_field] = universe.groupby('trade_date')[alpha_field].rank(method='min', pct=True).apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['alpha_AI'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Tickers\n",
    "fill data to history by 0 in order to get align ticker marix data each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_holding(df):\n",
    "    ticker_list = df.ts_code.unique()\n",
    "    calendar = df.trade_date.unique()\n",
    "    for dt in tqdm(calendar):\n",
    "        day_df = df.loc[df.trade_date == dt]\n",
    "        diff_tickers = list(set(ticker_list) - set(day_df.ts_code.unique()))\n",
    "        if len(diff_tickers) > 0:\n",
    "            for ticker in diff_tickers:\n",
    "                check_df = df.loc[df.ts_code==ticker].iloc[0,:]\n",
    "                # date ts_code trade_date name industry close log-ret  alpha_atr, alpha_010, alpha_149, alpha_AI\n",
    "                data = [pd.to_datetime(dt, format='%Y%m%d'), ticker, dt, check_df['name'],\n",
    "                        check_df['industry'], 0., 0., 0., 0., 0., 0.]\n",
    "                data = pd.DataFrame(data=data, index=np.append(['date'], df.columns)).T.set_index(['date'])\n",
    "                df = df.append(data)\n",
    "    df = df.sort_values(by=['date']) \n",
    "    return df\n",
    "\n",
    "universe = init_holding(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Exposures and Factor Returns\n",
    "The facort values in cross section should view as a type of exposure. We can calculate factor returns bettwen exposures of each ticker and daily return. We also did this in backtestig animate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shif return 5 days\n",
    "all_factors = universe.copy(deep=True)\n",
    "all_factors = all_factors.sort_values(by=['date'])\n",
    "def return_handle(df):\n",
    "    df['returns_2'] = df['log-ret'].shift(-5)\n",
    "    return df\n",
    "all_factors = all_factors.groupby('ts_code').apply(return_handle)\n",
    "all_factors = all_factors.replace([np.inf, -np.inf], np.nan).fillna(0.).sort_values(by=['date'])\n",
    "print(universe.shape, all_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    return np.where(x <= a,a, np.where(x >= b, b, x))\n",
    "\n",
    "def get_formula(factors, Y):\n",
    "    L = [\"0\"]\n",
    "    L.extend(factors)\n",
    "    return Y + \" ~ \" + \" + \".join(L)\n",
    "\n",
    "def factors_from_names(n, name):\n",
    "    return list(filter(lambda x: name in x, n))\n",
    "\n",
    "def estimate_factor_returns(df, name='alpha_'): \n",
    "    ## winsorize returns for fitting \n",
    "    estu = df.copy(deep=True)\n",
    "    estu['returns_2'] = wins(estu['returns_2'], -0.15, 0.15)\n",
    "    estu['alpha_AI'] = wins(estu['alpha_AI'], -0.175, 0.175)\n",
    "    all_factors = factors_from_names(list(df), name)\n",
    "    results = pd.Series()\n",
    "    for factor_name in all_factors:\n",
    "        form = get_formula([factor_name], \"returns_2\")\n",
    "        model = ols(form, data=estu)\n",
    "        result = model.fit()\n",
    "        results = results.append(result.params)\n",
    "    return results\n",
    "\n",
    "estimate_factor_returns(all_factors.loc[all_factors['trade_date']==20221010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "date_and_code = [ 'trade_date','ts_code', 'returns_2']\n",
    "calendar = all_factors.trade_date.unique() # int64\n",
    "alpha_df = all_factors[alpha_field + date_and_code]\n",
    "facret = {}\n",
    "for dt in tqdm(calendar, desc='regression factor returns'):\n",
    "    facret[dt] = estimate_factor_returns(alpha_df.loc[alpha_df['trade_date']==dt])\n",
    "facret[calendar[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veiw Factor Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = all_factors.index.unique()\n",
    "facret_df = pd.DataFrame(index = date_list)\n",
    "\n",
    "for ii, dt in zip(calendar,date_list): \n",
    "    for alp in alpha_field: \n",
    "        facret_df.at[dt, alp] = facret[ii][alp]\n",
    "\n",
    "for column in facret_df.columns:\n",
    "    plt.plot(facret_df[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Factor Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA model\n",
    "We use PCA algorithm to estimate risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class RiskModel(object):\n",
    "    def __init__(self, returns, num_factor_exposures, ann_factor=252):\n",
    "        \n",
    "        self.num_factor_exposures = num_factor_exposures\n",
    "        self.pca = PCA(n_components=num_factor_exposures, svd_solver='full')\n",
    "        self.pca.fit(returns)\n",
    "        \n",
    "        self.factor_betas_ = self.factor_betas(self.pca, returns.columns.values, np.arange(num_factor_exposures))\n",
    "        self.factor_returns_ = self.factor_returns(self.pca, returns, returns.index, np.arange(num_factor_exposures))\n",
    "        self.factor_cov_matrix_ = self.factor_cov_matrix(self.factor_returns_, ann_factor)\n",
    "        \n",
    "        self.idiosyncratic_var_matrix_ = self.idiosyncratic_var_matrix(returns, \n",
    "                                            self.factor_returns_, self.factor_betas_, ann_factor)\n",
    "        self.idiosyncratic_var_vector = pd.DataFrame(data=np.diag(self.idiosyncratic_var_matrix_),\n",
    "                                                     index=returns.columns)\n",
    "    \n",
    "    # got new exposure expressed by pca model\n",
    "    def factor_betas(self, pca, factor_beta_indices, factor_beta_columns):\n",
    "        return pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\n",
    "    \n",
    "    # got new factor returns expressed by pca model\n",
    "    def factor_returns(self, pca, returns, factor_return_indices, factor_return_columns):\n",
    "        return pd.DataFrame(pca.transform(returns), factor_return_indices, factor_return_columns)\n",
    "    \n",
    "    # got new factor covariance matirx by pca expressed returns\n",
    "    def factor_cov_matrix(self, factor_returns, ann_factor):\n",
    "        return np.diag(factor_returns.var(axis=0, ddof=1) * ann_factor)\n",
    "    \n",
    "    # calculate idiosyncratic need to got factor_returns, factor_betas which calculate by pca model first\n",
    "    def idiosyncratic_var_matrix(self, returns, factor_returns, factor_betas, ann_factor):\n",
    "        estimate_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n",
    "        residuals = returns - estimate_returns\n",
    "        return pd.DataFrame(np.diag(np.var(residuals))*ann_factor, returns.columns, returns.columns)\n",
    "    \n",
    "    def plot_principle_risk(self):\n",
    "        # Make the bar plot\n",
    "        plt.bar(np.arange(self.num_factor_exposures), self.pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_df_by_day(df, start_time):\n",
    "    pca_time_window = len(df.loc[df['trade_date']<start_time].trade_date.unique())\n",
    "    print(f'pca window_len is {pca_time_window}')\n",
    "    #trade_date_list = df.loc[df['trade_date']>=start_time].trade_date.unique()\n",
    "    all_date_list = df.trade_date.unique()\n",
    "    for start_i in range(len(all_date_list)):\n",
    "        start_date = all_date_list[start_i]\n",
    "        if start_i + pca_time_window >= len(all_date_list):\n",
    "            break\n",
    "        end_date = all_date_list[start_i + pca_time_window]\n",
    "        yield end_date, df.loc[(df['trade_date']>=start_date) & (df['trade_date']<=end_date)]\n",
    "        \n",
    "def risk_by_PCA(returns_df):\n",
    "    # Set the number of factor exposures (principal components) for the PCA algorithm\n",
    "    num_factor_exposures = 1\n",
    "    # Create a RiskModel object\n",
    "    rm = RiskModel(returns_df, num_factor_exposures)\n",
    "    \n",
    "    B = rm.factor_betas_\n",
    "    F = rm.factor_cov_matrix_\n",
    "    S = rm.idiosyncratic_var_matrix_\n",
    "    f = rm.factor_returns_\n",
    "    \n",
    "    variance = np.dot(B, F).dot(B.T) + S\n",
    "    return variance, B, f, rm.idiosyncratic_var_vector\n",
    "\n",
    "# test\n",
    "start_time = 20221101\n",
    "dt, df = next(rolling_df_by_day(all_factors, start_time))\n",
    "returns_df = df.pivot(index='trade_date', columns='ts_code', values='log-ret').fillna(0)\n",
    "variance_i, B, risk_factor, residual_i = risk_by_PCA(returns_df)\n",
    "print(f'return date {dt}')\n",
    "variance_i.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array([1]*147) \n",
    "(np.dot(h, variance_i).dot(h.T))** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 20221101\n",
    "variance_all = {}\n",
    "residual_df = pd.DataFrame()\n",
    "\n",
    "for dt, df in rolling_df_by_day(all_factors, start_time):\n",
    "    ticker_list = all_factors.loc[all_factors.trade_date==dt].ts_code.unique()\n",
    "    df = df.loc[df.ts_code.isin(ticker_list)]\n",
    "    returns_df = df.pivot(index='trade_date', columns='ts_code', values='log-ret').fillna(0)\n",
    "    variance_i, B, risk_factor, residual_i = risk_by_PCA(returns_df)\n",
    "    variance_all[dt] = [variance_i, B, risk_factor.mean(axis=0)]\n",
    "    residual_i['trade_date'] = df.loc[df.index[-1],'trade_date'].unique()[-1]\n",
    "    residual_df = residual_df.append(residual_i)\n",
    "\n",
    "residual_df.reset_index(inplace=True)\n",
    "residual_df.columns = ['ts_code', 'residual', 'trade_date']\n",
    "residual_df['residual'] = np.where(residual_df['residual'].isnull(), residual_df['residual'].median(), residual_df['residual'])\n",
    "all_factors = all_factors.loc[all_factors['trade_date']>=start_time]\n",
    "all_factors = all_factors.merge(residual_df, on=['trade_date','ts_code'], how='left')\n",
    "#all_factors.tail()\n",
    "print(residual_df.shape, all_factors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(all_factors['alpha_atr']), min(all_factors['alpha_atr']))\n",
    "print(max(all_factors['alpha_149']), min(all_factors['alpha_149']))\n",
    "print(max(all_factors['alpha_010']), min(all_factors['alpha_010']))\n",
    "print(max(all_factors['alpha_AI']), min(all_factors['alpha_AI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale factor value from -1 to 1\n",
    "for factor_name in alpha_field:\n",
    "    all_factors[factor_name] = all_factors[factor_name]/1.7\n",
    "    all_factors[factor_name] = wins(all_factors[factor_name], -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['date'] = pd.to_datetime(all_factors['trade_date'], format='%Y%m%d')\n",
    "alpha_df = all_factors.drop(columns=['returns_2']).set_index(['date', 'ts_code'])\n",
    "alpha_df['alpha_all'] = 0.7 * alpha_df['alpha_AI'] + 0.3*alpha_df['alpha_149'] + 0.1*alpha_df['alpha_010']\n",
    "alpha_df = alpha_df.sort_values(by=['date'])                   \n",
    "print(max(alpha_df['alpha_all']), min(alpha_df['alpha_all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest by Convex Optimization\n",
    "- use `scipy.optimize.fmin_l_bfgs_b` top optimized portfolio\n",
    "- lambda expressed a transaction costs weights\n",
    "- aversion expressed a risk rescale weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lambda expressed a transaction costs weights\n",
    "def get_obj_func(h0, variance, alpha, Lambda=1e-3, aversion=0.005): \n",
    "    def obj_func(h):\n",
    "        #f = 0.5 * aversion * np.dot(h, variance).dot(h.T) - aversion * np.matmul(h, alpha) + np.dot((h-h0)**2, Lambda)\n",
    "        f =  - np.matmul(h, alpha) \\\n",
    "             + 0.5 * aversion * np.dot(h, variance).dot(h.T) \\\n",
    "             + Lambda * abs((h-h0))\n",
    "        return f\n",
    "    return obj_func\n",
    "\n",
    "def get_grad_func(h0, variance, alpha, Lambda=1e-3, aversion=0.005):\n",
    "    def grad_func(h):\n",
    "        #f_hat =  aversion * np.dot(variance, h) - aversion * alpha + 2 * Lambda * (h-h0)\n",
    "        f_hat =  - alpha  \\\n",
    "                + aversion * np.dot(variance, h) \\\n",
    "                + Lambda\n",
    "        return f_hat\n",
    "    return grad_func\n",
    "\n",
    "# bounds means position bounds at each row. Here I only trade for long so that bounds from 0 to 1\n",
    "\n",
    "#start_time = 20220315\n",
    "alpha_df['h_privious'] = 0.\n",
    "positions = {}\n",
    "calendar = alpha_df.trade_date.unique()\n",
    "\n",
    "# get parameter\n",
    "ticker_num = len(alpha_df.index.get_level_values(1).unique())\n",
    "h0 = [0.] * ticker_num\n",
    "bounds = [(0,1)] * ticker_num\n",
    "\n",
    "for dt in calendar:\n",
    "    # fill yesterday holding\n",
    "    obj_df = alpha_df.loc[alpha_df.trade_date==dt]\n",
    "    \n",
    "    # convex optimize\n",
    "    obj_func = get_obj_func(h0, variance_all[dt][0], obj_df['alpha_all'].values)\n",
    "    grad_func = get_grad_func(h0, variance_all[dt][0], obj_df['alpha_all'].values)\n",
    "    h_optimal, min_val, _ = fmin_l_bfgs_b(obj_func, h0, fprime=grad_func, bounds=bounds)\n",
    "    #h_optimal, min_val, _ = fmin_l_bfgs_b(obj_func, h0, fprime=grad_func)\n",
    "    \n",
    "    # update optimize holding\n",
    "    obj_df['h_opt'] = h_optimal\n",
    "    obj_df['h_privious'] = h0\n",
    "    positions[dt]= obj_df\n",
    "    h0 = h_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = []\n",
    "for dt in list(positions.keys())[:-4]:\n",
    "    all_positions.append(positions[dt].h_opt.sum())\n",
    "print(max(all_positions), min(all_positions))\n",
    "#positions[20230315]['h_opt'].hist()\n",
    "pd.Series(all_positions).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize optimal sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in positions.keys():\n",
    "    positions[i]['h_opt'] = positions[i]['h_opt']/(positions[i]['h_opt'].sum() + 1e-6)\n",
    "    positions[i]['h_privious'] = positions[i]['h_privious']/(positions[i]['h_privious'].sum()+ 1e-6)\n",
    "h_optimal_list = [positions[dt]['h_opt'] for dt in positions.keys() ]\n",
    "h_privious_list = [positions[dt]['h_privious'] for dt in positions.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = []\n",
    "for dt in list(positions.keys())[:-4]:\n",
    "    all_positions.append(positions[dt].h_opt.sum())\n",
    "print(max(all_positions), min(all_positions))\n",
    "#positions[20230315]['h_opt'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Exposure and Transaction Costs\n",
    "We use pca to calculate risk, so we can view residual(alpha) as risk exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = [7, 3]\n",
    "risk_exposures_df = pd.DataFrame()\n",
    "risk_exposures = {}\n",
    "for ii, dt in enumerate(positions.keys()):\n",
    "    B = variance_all[dt][1]\n",
    "    h_opt_i = h_optimal_list[ii]\n",
    "    risk_exposure = np.matmul(h_opt_i.T, B)\n",
    "    risk_exposures[dt] = risk_exposure\n",
    "    risk_exposures_df = risk_exposures_df.append(risk_exposure, ignore_index=True)\n",
    "\n",
    "#np.sum(risk_exposures)\n",
    "risk_exposures_df.plot(grid=True, title='Risk Exposure')\n",
    "#risk_exposures_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import pandas\n",
    "\n",
    "def colnames(B):\n",
    "    if type(B) == patsy.design_info.DesignMatrix: \n",
    "        return B.design_info.column_names\n",
    "    if type(B) == pandas.core.frame.DataFrame: \n",
    "        return B.columns.tolist()\n",
    "    return None\n",
    "\n",
    "def get_B_alpha(universe):\n",
    "    alpha_factors = factors_from_names(list(universe),'alpha_')\n",
    "    formula = get_formula(alpha_factors, \"returns_2\")\n",
    "    outcome, B_alpha = patsy.dmatrices(formula, universe)\n",
    "    return B_alpha\n",
    "\n",
    "alpha_df_ = alpha_df.merge(all_factors[['trade_date','ts_code','returns_2']], on=['trade_date','ts_code'], how='left')\n",
    "calendar = alpha_df.trade_date.unique()\n",
    "alpha_exposure_df = pd.DataFrame()\n",
    "alpha_exposures = {}\n",
    "for ii, dt in enumerate(calendar[:-4]):\n",
    "    alpha_df_i = alpha_df_.loc[alpha_df_.trade_date==dt][['trade_date','ts_code','alpha_AI', 'alpha_149', 'alpha_010', 'alpha_atr', 'returns_2']]\n",
    "    h_opt_i = h_privious_list[ii]\n",
    "    B_alpha = get_B_alpha(alpha_df_i)\n",
    "    \n",
    "    # alpha_df['alpha_all'] = 0.7 * alpha_df['alpha_AI'] + 0.3*alpha_df['alpha_149'] + 0.1*alpha_df['alpha_010']\n",
    "    B_alpha = B_alpha * [[0.7, 0.2, 0.1, 0.]] #* len(alpha_df_.ts_code.unique())\n",
    "    #alpha_exposure = pd.Series(np.matmul(B_alpha.transpose(), h_opt_i), index=colnames(B_alpha))\n",
    "    alpha_exposure = pd.Series(np.matmul(B_alpha.transpose(), h_opt_i), index=['alpha_AI', 'alpha_149', 'alpha_010', 'alpha_atr'])\n",
    "    alpha_exposures[dt] = alpha_exposure\n",
    "    alpha_exposure_df = pd.concat([alpha_exposure_df,alpha_exposure], axis=1)\n",
    "\n",
    "alpha_exposure_df = alpha_exposure_df.T.reset_index(drop=True)\n",
    "alpha_exposure_df.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_costs = []\n",
    "Lambda = 1e-3\n",
    "for i in range(len(h_optimal_list)-4):\n",
    "    tmp_change = h_optimal_list[i] - h_privious_list[i]\n",
    "    #costs = sum(np.dot(tmp_change.values**2, Lambda))\n",
    "    costs = Lambda * abs(tmp_change.values).sum()\n",
    "    transaction_costs.append(costs)\n",
    "print(sum(transaction_costs))\n",
    "plt.plot(transaction_costs[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit-and-Loss (PnL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['date'] = pd.to_datetime(all_factors['trade_date'], format='%Y%m%d')\n",
    "all_factors = all_factors.set_index(['date']).sort_values(by=['date'])\n",
    "\n",
    "## assumes v, w are pandas Series \n",
    "def partial_dot_product(v, w):\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution(): \n",
    "\n",
    "    df = pd.DataFrame(index = pd.to_datetime(all_factors['trade_date'].unique(), format='%Y%m%d')).iloc[:-4,:]\n",
    "    calendar = all_factors.trade_date.unique()[:-4]\n",
    "    counter = range(len(calendar))\n",
    "    for ii, dt, time_i in zip(counter,calendar,df.index):\n",
    "        # holding frame\n",
    "        p = positions[dt]\n",
    "        # alpha f\n",
    "        #fr = facret[dt][[0,2,5]]\n",
    "        fr = facret[dt].loc[['alpha_AI', 'alpha_149', 'alpha_010', 'alpha_atr']]\n",
    "        # risk f\n",
    "        rr = variance_all[dt][2]\n",
    "        row_universe = all_factors.loc[all_factors.trade_date==dt]\n",
    "        mf = p[['h_privious', 'h_opt']].merge(row_universe[['ts_code', 'returns_2']], how = 'left', on = \"ts_code\")  \n",
    "        mf['returns_2'] = wins(mf['returns_2'], -0.2, 0.2)\n",
    "        df.at[time_i,\"time.pnl\"] = np.sum(mf['h_opt'] * mf['returns_2'])\n",
    "        df.at[time_i,\"alpha.pnl\"] = partial_dot_product(fr, alpha_exposures[dt])\n",
    "        df.at[time_i,\"risk.pnl\"] = partial_dot_product(rr, risk_exposures[dt])\n",
    "        df.at[time_i,\"cost\"] = transaction_costs[ii]\n",
    "    \n",
    "    print(time_i)\n",
    "    return df\n",
    "\n",
    "attr = build_pnl_attribution()\n",
    "for column in attr.columns:\n",
    "    plt.plot(attr[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PnL Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facret[20221101].loc[['alpha_AI', 'alpha_149', 'alpha_010', 'alpha_atr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_exposures[20221101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
