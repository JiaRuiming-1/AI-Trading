{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "In this notebook, We use data which I donwload from Tushare to make up some alpha factors and risk factors.\n",
    "1. Load data which time range bettwen 2017.4 - 2023.3. download from privious step `DownLoad_from_Tushare.ipynb` \n",
    "2. Make up some factors based on some paper, tech indicators and fundamentls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "load data which already download from Tushare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universe_raw = pd.read_csv('all_20170405_20230317.csv')\n",
    "#fundamental_df = pd.read_csv('fundamental_20170405_20230317.csv').iloc[:,1:]\n",
    "universe_raw = pd.read_csv('tmp.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load recent data\n",
    "universe_recent = pd.read_csv('raw_20230103_20230327.csv').iloc[:,1:]\n",
    "universe_recent = universe_recent.loc[universe_recent.ts_code.isin(fundamental_df.ts_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(universe_raw.ts_code.unique()), len(universe_recent.ts_code.unique()))\n",
    "print(len(universe_raw.columns.unique()), len(universe_recent.columns.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = universe_raw.copy(deep=True)\n",
    "# reset pe is null to 1.e3\n",
    "universe['pe'] = np.where(universe['pe'].isnull(), 1.*1e3, universe['pe'])\n",
    "# fill pb null with a large value\n",
    "universe['pb'] = np.where(universe['pb'].isnull(), 1.*1e2, universe['pb'])\n",
    "# fill null to 0\n",
    "universe['dt_eps'] = np.where(universe['dt_eps'].isnull(), 0., universe['dt_eps'])\n",
    "universe['dt_eps_yoy'] = np.where(universe['dt_eps_yoy'].isnull(), 0., universe['dt_eps_yoy'])\n",
    "universe['dt_eps_yoy'] = np.where(universe['dt_eps_yoy']>300, 300., np.where(universe['dt_eps_yoy']<-300, -300, universe['dt_eps_yoy']))\n",
    "universe['p_change_max'] = np.where(universe['p_change_max'].isnull(), 0., universe['p_change_max'])\n",
    "universe['p_change_max'] = np.where(universe['p_change_max']>300, 300., np.where(universe['p_change_max']<-300, -300, universe['p_change_max']))\n",
    "# add volume\n",
    "universe['volume'] = universe['amount']/universe['close']\n",
    "universe = universe.drop_duplicates(['trade_date','ts_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explain\n",
    "  Some data columns about fundamental indicators below\n",
    "  - 'cfps','revenue_ps', 'quick_ratio',  每股现金流，每股营业收入，速冻比率\n",
    "  - 'dt_eps','basic_eps_yoy','dt_eps_yoy',  每股收益\n",
    "  - 'bps','bps_yoy',  每股净资产\n",
    "  - 'extra_item','profit_dedt', 扣非，扣非净利润\n",
    "  - 'roe_dt','q_dt_roe','roe_yoy',  净资产收益\n",
    "  - 'capital_rese_ps','surplus_rese_ps',  每股资本公积，每股公积盈余 \n",
    "  - 'gross_margin','interestdebt','ca_to_assets', 毛利，带息债务, 流动资产/总资产\n",
    "  - 'ebt_yoy','roe_yoy','or_yoy','equity_yoy' 总利润增长，净资产收益增长，营业收入增长，净资产增长\n",
    "  \n",
    "  If there are some columns named alpha begin, the data had processed completed. Each column named alpha_ is alpha factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker Pool\n",
    "After create factors we filte data by each day more in detail. \n",
    "\n",
    "In real trade with this model factor use, download data by each day, if you want to add new tickers into portfolio, ensure the ticker exist over 3 month. If add not successed after this process step, some conditions not satisfied our portfolio regular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter tikers: 100%|████████████████████████████████████████████████████████████████| 243/243 [00:01<00:00, 130.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9833, 57) 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove tickers by each day not exist history\n",
    "def remove_tickers(df, exist_ticker_list):\n",
    "    diff_df = df.loc[df.ts_code.isin(exist_ticker_list)==False]\n",
    "    if diff_df.empty == False:\n",
    "        # remove tickers not exist over 3month from day\n",
    "        diff1 = (pd.to_datetime(diff_df['trade_date'],format='%Y%m%d')\\\n",
    "                -pd.to_datetime(diff_df['list_date'],format='%Y%m%d')).apply(lambda x: x.days) < 90\n",
    "        # remove tickers pe > 60\n",
    "        diff2 = diff_df.pe > 60\n",
    "        # remove tickers pb > 10\n",
    "        diff3 = diff_df.pb > 8\n",
    "        # remove fundamental bad\n",
    "        diff4 = diff_df['type_value']<-1\n",
    "        # dt_eps <0 means profit is below 0.3 , remove it.\n",
    "        diff5 = diff_df['dt_eps']<0.3\n",
    "        # total_mv>50 billion\n",
    "        diff6 = diff_df['total_mv']>5000000\n",
    "        # get remove df\n",
    "        diff_df = diff_df.loc[diff1| diff2| diff3| diff4| diff5| diff6]\n",
    "        # get rest data\n",
    "        df = df.loc[df.ts_code.isin(diff_df.ts_code)==False]\n",
    "    return df\n",
    "\n",
    "calendar = universe.trade_date.unique()\n",
    "df_all = pd.DataFrame()\n",
    "for dt in tqdm(calendar, desc='filter tikers'):\n",
    "    tmp = universe.loc[universe['trade_date']==dt]\n",
    "    if df_all.empty:\n",
    "        tmp = remove_tickers(tmp, [])\n",
    "    else:\n",
    "        tmp = remove_tickers(tmp, df_all.ts_code.unique())\n",
    "    df_all = df_all.append(tmp, ignore_index=True)\n",
    "universe = df_all\n",
    "universe['date'] = pd.to_datetime(universe['trade_date'], format='%Y%m%d')\n",
    "universe = universe.set_index(['date']).sort_values(by=['date'])\n",
    "# update stock list\n",
    "#fundamental_df = fundamental_df.loc[fundamental_df.ts_code.isin(universe.ts_code.unique())]\n",
    "print(universe.shape, len(universe.ts_code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = ['ts_code', 'trade_date', 'turnover_rate', 'amount', 'pe', 'pb', 'total_share', 'total_mv', 'volume',\n",
    "         'open', 'close', 'high', 'low', 'cci', \n",
    "         'name', 'industry', 'list_date', 'issue_price', 'issue_amount',\n",
    "         'type', 'type_value', 'p_change_min','p_change_max', \n",
    "         'cfps', 'revenue_ps', 'quick_ratio', 'dt_eps', 'basic_eps_yoy', 'dt_eps_yoy',\n",
    "         'bps', 'bps_yoy', 'extra_item', 'profit_dedt', 'roe_dt', 'q_dt_roe',\n",
    "         'roe_yoy', 'capital_rese_ps', 'surplus_rese_ps', 'gross_margin',\n",
    "         'interestdebt', 'ca_to_assets', 'ebt_yoy', 'or_yoy', 'equity_yoy']\n",
    "universe = universe[field]\n",
    "#universe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overnight Return\n",
    "This factor we had did in P4, \n",
    "\n",
    "We calculate factor here, and add sma indicators later.\n",
    "\n",
    "$factor=(open_{today}-close_{yesterday})\\div close_{yesterday}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "close_to_open: 100%|██████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 314.09it/s]\n"
     ]
    }
   ],
   "source": [
    "class CloseToOpen(pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Overnight Return Factor Constructor\n",
    "    \"\"\"\n",
    "    def __init__(self,data):\n",
    "        super(CloseToOpen, self).__init__(data)\n",
    "        self.df = self\n",
    "\n",
    "    def calculate(self):\n",
    "        '''\n",
    "        add open-close as a column named close_to_return\n",
    "        :return: dataframe\n",
    "        '''\n",
    "        unique_ticker = self.df.ts_code.unique()\n",
    "        tmp_df = pd.DataFrame()\n",
    "        for ts_code in tqdm(unique_ticker, desc='close_to_open'):\n",
    "            stock = self.df.loc[self.df.ts_code == ts_code][[\"ts_code\", \"trade_date\", \"open\", \"close\"]]\n",
    "            stock['alpha_close2open'] = (stock['open'].shift(-1).fillna(method='ffill') - stock['close'])/stock['close']\n",
    "            tmp_df = tmp_df.append(stock, ignore_index=True)\n",
    "        self.df = self.df.merge(tmp_df[[\"ts_code\", \"trade_date\", \"alpha_close2open\"]], on=[\"ts_code\", \"trade_date\"], how=\"inner\")\n",
    "        self.df['date'] = pd.to_datetime(self.df['trade_date'], format='%Y%m%d')\n",
    "        self.df = self.df.set_index(['date']).sort_values(by=['date'])\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "universe = CloseToOpen(universe).calculate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Technology Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stockstats\n",
    "\n",
    "class IndicatorHelper(pd.DataFrame):\n",
    "    \"\"\"\n",
    "        add indicators to dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(IndicatorHelper, self).__init__(data)\n",
    "\n",
    "        self.stocks = stockstats.StockDataFrame.retype(data.copy())\n",
    "        self.df = self\n",
    "\n",
    "    def add_technical_indicator(self, tech_indicator_list):\n",
    "        \"\"\"\n",
    "        calculate technical indicators\n",
    "        use stockstats package to add technical inidactors\n",
    "        :param ticker: (df) pandas dataframe\n",
    "        :param tech_indeicator_list list\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        unique_ticker = self.df.ts_code.unique()\n",
    "\n",
    "        indicator_df = pd.DataFrame()\n",
    "        for i in tqdm(range(len(unique_ticker)), desc='add tech indicators'):\n",
    "            temp_indicator = self.stocks[self.stocks.ts_code == unique_ticker[i]]\n",
    "            temp_indicator = temp_indicator[tech_indicator_list + ['ts_code','trade_date']]\n",
    "            indicator_df = indicator_df.append(temp_indicator, ignore_index=True)\n",
    "        \n",
    "        self.df = self.df.merge(indicator_df, on=[\"ts_code\", \"trade_date\"], how=\"left\")\n",
    "        self.df['date'] = pd.to_datetime(self.df['trade_date'],format='%Y%m%d')\n",
    "        self.df = self.df.set_index(['date']).sort_values(by=['date']).drop_duplicates(['trade_date','ts_code'])\n",
    "        return self.df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "add tech indicators: 100%|█████████████████████████████████████████████████████████████| 52/52 [00:06<00:00,  7.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# add tech indicators\n",
    "tech_indicator_list = [\n",
    "                       'supertrend','close_5_sma','close_20_sma','turnover_rate_5_sma',\n",
    "                       'log-ret','atr_5','cci_6', 'vwma_2','vwma_25',\n",
    "                       'close_10_kama_2_30', 'close_10_kama_5_30','close_2_kama',\n",
    "                       'alpha_close2open_5_sma', 'alpha_close2open_25_sma'\n",
    "                      ] \n",
    "universe = IndicatorHelper(universe).add_technical_indicator(tech_indicator_list)\n",
    "# 'close_2_kama_20_mstd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor Factors Based on Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supertrend Factors\n",
    "This factor based on supertrend and close 5 days sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['alpha_supertrend'] = universe['close'] - universe['supertrend']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCI Factors\n",
    "This factor based on cci and atr 5 days window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['alpha_cci'] = np.where(universe['cci_6']>200, (universe['cci_6']-200)/universe['turnover_rate_5_sma'], \\\n",
    "                      np.where(universe['cci_6']<-200, (universe['cci_6']+200)/universe['turnover_rate_5_sma'], universe['turnover_rate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAMA Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['close_2_kama'] = \\\n",
    "    np.where(universe['close_2_kama'].isnull()==True,universe['close_5_sma'], universe['close_2_kama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kama filter: 100%|████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 219.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# add KAMA alpha factor\n",
    "def KAMA_filter(df):    \n",
    "    unique_stocks = df.ts_code.unique()\n",
    "    all_df = pd.DataFrame()\n",
    "    for ts_code in tqdm(unique_stocks, desc='kama filter'):\n",
    "        tmp = df.loc[df.ts_code == ts_code]\n",
    "        tmp['kama_filter'] = tmp['close_2_kama'].rolling(window=20).std().fillna(method='bfill') \n",
    "        tmp['kama_prior'] = tmp['close_2_kama'].shift(5).fillna(method='bfill')\n",
    "        #tmp['alpha_kama'] = (tmp['close_2_kama'] - tmp['kama_prior'] - tmp['kama_filter'])+ (tmp['close_10_kama_2_30'] - tmp['close_10_kama_5_30'])\n",
    "        tmp['alpha_kama'] = tmp['close_2_kama'] - tmp['kama_prior'] - tmp['kama_filter']\n",
    "        all_df = all_df.append(tmp[['ts_code','trade_date','alpha_kama']], ignore_index=True)\n",
    "    df = df.merge(all_df, on=['ts_code','trade_date'], how='left')\n",
    "    df['date'] = pd.to_datetime(df['trade_date'],format='%Y%m%d')\n",
    "    df = df.set_index(['date']).sort_values(by=['date'])\n",
    "    return df\n",
    "\n",
    "#temp = universe.loc[universe.ts_code=='603538.SH']\n",
    "#temp = KAMA_filter(temp)\n",
    "universe = KAMA_filter(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor Factors Based on Paper\n",
    "### Overnight Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had add overnight return factors\n",
    "universe['alpha_close2open_25_sma'] = -universe['alpha_close2open_25_sma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winner And Loser\n",
    "This factor we also did in P4. It express a ticker how to reach a return in a spicific period time \n",
    "\n",
    "We use a time window as T, and regression d and v $return = T*d + T^2*v$  => $factor=d*v$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "winner and loser: 100%|████████████████████████████████████████████████████████████████| 52/52 [01:07<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "class WinnerAndLoser(pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Winner and Loser Factor Constructor\n",
    "    \"\"\"\n",
    "    def __init__(self,data, win_length=20):\n",
    "        super(WinnerAndLoser, self).__init__(data)\n",
    "        self.df = self\n",
    "        self.win_lenth = win_length\n",
    "\n",
    "    def _regression(self, data):\n",
    "        df = pd.DataFrame(data, columns=['log-ret'])\n",
    "        df['acc_ret'] = df['log-ret'].cumsum()\n",
    "        df['t_dir'] = np.arange(self.win_lenth)+1\n",
    "        df['t_velocity'] = df['t_dir'] ** 2\n",
    "        regression = ols(formula='acc_ret ~ 0 + t_dir + t_velocity', data=df)\n",
    "        model = regression.fit()\n",
    "        data['alpha_winlos'] = model.params.t_dir * abs(model.params.t_velocity)\n",
    "        return  data['alpha_winlos']\n",
    "\n",
    "\n",
    "    def calculate(self):\n",
    "        '''\n",
    "        convert time to value\n",
    "        regress return to get mu and beta each time\n",
    "        add facotor mu*beta to colomns\n",
    "        :return: dataframe\n",
    "        '''\n",
    "        tickers = self.df.ts_code.unique()\n",
    "        factor_df = pd.DataFrame()\n",
    "        for ticker in tqdm(tickers, desc='winner and loser'):\n",
    "            tmp_df = self.df.loc[self.df.ts_code == ticker][['trade_date', 'ts_code', 'log-ret']]\n",
    "            tmp_df['alpha_winlos'] = tmp_df['log-ret'].rolling(self.win_lenth).apply(self._regression)\n",
    "            tmp_df['alpha_winlos'].fillna(method='bfill',inplace=True)\n",
    "            factor_df = factor_df.append(tmp_df, ignore_index=True)\n",
    "        self.df = self.df.merge(factor_df[[\"ts_code\", \"trade_date\", \"alpha_winlos\"]], on=[\"ts_code\", \"trade_date\"], how=\"left\")\n",
    "        self.df['date'] = pd.to_datetime(self.df['trade_date'],format='%Y%m%d')\n",
    "        self.df = self.df.set_index(['date']).sort_values(by=['date'])\n",
    "        return  self.df\n",
    "    \n",
    "    \n",
    "#test = universe.loc[universe.ts_code=='603538.SH']\n",
    "#test = universe.loc[universe.ts_code=='002038.SZ']\n",
    "#test = WinnerAndLoser(test).calculate()\n",
    "universe = WinnerAndLoser(universe).calculate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew And Momentum\n",
    "This factor we also did in P4. It express minority and majority sentiment of investor how to impact on market.\n",
    "\n",
    "We calculate skew and median of log-return distribution in a period time, the skew view as marjority sentiment and median can view as minority sentiment.\n",
    "\n",
    "$factor = abs(skew) * median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skew and momentum: 100%|███████████████████████████████████████████████████████████████| 52/52 [00:02<00:00, 18.70it/s]\n"
     ]
    }
   ],
   "source": [
    "class SkewandMomentum(pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Expected Skewness and Momentum Factor Constructor\n",
    "    \"\"\"\n",
    "    def __init__(self,data, win_length=20):\n",
    "        super(SkewandMomentum, self).__init__(data)\n",
    "        self.df = self\n",
    "        self.win_length = win_length\n",
    "\n",
    "    def calculate(self):\n",
    "        '''\n",
    "        convert time to value\n",
    "        regress return to get mu and beta each time\n",
    "        add facotor mu*beta to colomns\n",
    "        :return: dataframe\n",
    "        '''\n",
    "        def calculate_factor(data):\n",
    "            return data.skew() * data.median()\n",
    "        \n",
    "        tmp_df = pd.DataFrame()\n",
    "        for stock_tuple in tqdm(self.groupby('ts_code'), desc='skew and momentum'):\n",
    "            stock = stock_tuple[1]\n",
    "            stock['alpha_skew2sentiment'] = stock['log-ret'].rolling(self.win_length).apply(calculate_factor)\n",
    "            stock['alpha_skew2sentiment'] = stock['alpha_skew2sentiment'].fillna(method='bfill')\n",
    "            tmp_df = tmp_df.append(stock,ignore_index=True)\n",
    "        self.df = self.df.merge(tmp_df[[\"ts_code\", \"trade_date\", \"alpha_skew2sentiment\"]], on=[\"ts_code\", \"trade_date\"], how=\"left\")\n",
    "        self.df['date'] = pd.to_datetime(self.df['trade_date'],format='%Y%m%d')\n",
    "        self.df = self.df.set_index(['date']).sort_values(by=['date'])\n",
    "        return self.df\n",
    "\n",
    "\n",
    "#temp = universe.loc[universe.ts_code=='603538.SH']\n",
    "#test = universe.loc[universe.ts_code=='002038.SZ']\n",
    "#temp = SkewandMomentum(temp).calculate()\n",
    "universe = SkewandMomentum(universe).calculate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Factor\n",
    "This factor based on ticker fundamentals, it usually take a long period time to archive return. So called take a long line to catch a big fish! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fundamental factor: 100%|█████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 216.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def alpha_fundamental(df):\n",
    "    all_df = pd.DataFrame()\n",
    "    for ts_code in tqdm(df.ts_code.unique(), desc='fundamental factor'):\n",
    "        tmp = df.loc[df.ts_code==ts_code]\n",
    "        tmp['alpha_fundamental'] = tmp['revenue_ps']/tmp['close'] * 30/tmp['pe']\n",
    "        all_df = all_df.append(tmp, ignore_index=True)\n",
    "        \n",
    "    df = df.merge(all_df[['ts_code','trade_date','alpha_fundamental']], on=['ts_code','trade_date'], how='left')\n",
    "    df['date'] = pd.to_datetime(df['trade_date'],format='%Y%m%d')\n",
    "    df = df.set_index(['date']).sort_values(by=['date'])\n",
    "    return df\n",
    "   \n",
    "\n",
    "#temp = universe.loc[universe.ts_code=='603538.SH']\n",
    "#temp = alpha_fundamental(temp)\n",
    "#universe = universe.drop(columns=['alpha_fundamental'])\n",
    "universe = alpha_fundamental(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zscore Alpha Factors\n",
    "As computer use factor to compare each oter before all factors had same scales. So we rescale each factors by zscore in each day and each industry. \n",
    "\n",
    "Here we pick some features as mechine learning step in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some feature distribution eg.'p_change_min','p_change_max'\n",
    "# universe[['dt_eps_yoy']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# add alpha_winlos columns if run this factor\n",
    "zscore_features = [\n",
    "       'turnover_rate', 'amount', 'total_mv', 'atr_5', 'pe', 'pb', 'revenue_ps', 'p_change_max', \n",
    "       'dt_eps_yoy', 'bps_yoy', 'roe_yoy', 'ebt_yoy', 'or_yoy', 'equity_yoy',\n",
    "       'alpha_cci', 'alpha_supertrend', 'alpha_kama', 'alpha_close2open_5_sma',\n",
    "       'alpha_close2open_25_sma', 'alpha_skew2sentiment', 'alpha_fundamental',\n",
    "       # 'alpha_winlos'\n",
    "        ]\n",
    "\n",
    "universe_raw = pd.DataFrame()\n",
    "for industry in universe.industry.unique():\n",
    "    tmp = universe.loc[universe.industry == industry]\n",
    "    tmp[zscore_features] = tmp.groupby('trade_date')[zscore_features].apply(zscore)\n",
    "    universe_raw = universe_raw.append(tmp)\n",
    "\n",
    "universe_raw.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_raw[zscore_features+['ts_code', 'industry']].sort_values(by=['date', 'pe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Data and factors to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add recent data\n",
    "print(len(universe_raw.columns.unique()), len(universe.columns.unique()))\n",
    "print(universe_raw.index[-1], universe.index[-1])\n",
    "# universe_raw = universe_raw.drop(columns=['rsi_6']).set_index(['date'])\n",
    "# for col in universe_raw.columns:\n",
    "#     if col not in universe.columns:\n",
    "#         print(col)\n",
    "universe_raw = universe_raw.append(universe.loc[universe['trade_date']>20230317])\n",
    "fundamental_df = fundamental_df.loc[fundamental_df.ts_code.isin(universe_raw.ts_code.unique())]\n",
    "print(len(universe_raw.ts_code.unique()), len(fundamental_df.ts_code.unique()))\n",
    "universe_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data\n",
    "# fundamental_df.to_csv('fundamental_20170405_20230327.csv')\n",
    "# universe_raw[features].to_csv('factors_20170405_20230327.csv')\n",
    "universe_raw.to_csv('all_20170405_20230327.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_all.loc[tmp_all['trade_date']==20230327].sort_values(by=['rank']).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something test here\n",
    "# 002788.SZ 000705.SZ |002317.SZ\n",
    "from scipy.stats import zscore\n",
    "tmp = universe_raw.loc[universe_raw.ts_code== '603538.SH']\n",
    "#tmp = tmp.loc[(tmp['trade_date']>20230101) & (tmp['trade_date']<20231101) ]\n",
    "tmp[['close','cci_6']] = tmp[['close', 'cci_6']].apply(zscore).fillna(method='bfill')\n",
    "#tmp['close'] = tmp['close']\n",
    "#tmp[['close', 'alpha_fundamental']].plot(grid=True)\n",
    "#tmp[['vwma_5', 'turnover_rate']].plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
