{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "In this notebook, we load data from privious step in `Factor_Process_and_Evaluate.ipynb` and we combine all factors by RandomForest method. Then we can evaluate all factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import project_helper as ph\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load universe_factors\n",
    "#fundamental_df = pd.read_csv('fundamental_20170405_20230317.csv').iloc[:,1:]\n",
    "universe = pd.read_csv('all_20170405_20230327.csv')\n",
    "universe['date'] = pd.to_datetime(universe['date'],format='%Y-%m-%d')\n",
    "universe.set_index(['date'],inplace=True)\n",
    "#print(fundamental_df.shape, universe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Sectors\n",
    "For the model to better understand the sector data, we'll one hot encode this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_dict = {'化学制药':1, '医药商业':2, '中成药':3, '生物制药':4}\n",
    "\n",
    "all_factors = universe.copy(deep=True)\n",
    "sector_columns = []\n",
    "for sector_name, sector_i in sector_dict.items():\n",
    "    all_factors['sector_{}'.format(sector_i)] = False\n",
    "    sector_columns.append('sector_{}'.format(sector_i))\n",
    "    \n",
    "for sector_name, sector_i in sector_dict.items():\n",
    "    all_factors['sector_{}'.format(sector_i)] = np.where(all_factors['industry']==sector_name, True, False)\n",
    "\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "Let's try to predict the go forward 1-week return. When doing this, it's important to quantize the target. The factor we create is the trailing 5-day return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_jud = all_factors['log-ret'].quantile([0.30,0.7,1.])\n",
    "quantile_jud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return_quantiles(all_factors):\n",
    "    all_factors['return_2q'] = 0\n",
    "    all_factors['return_2q'] = np.where(all_factors['log-ret']<=-0.01, -1, all_factors['return_2q'])\n",
    "    all_factors['return_2q'] = np.where(all_factors['log-ret']>=0.01, 1 , all_factors['return_2q'])\n",
    "        \n",
    "    return all_factors\n",
    "\n",
    "all_factors = get_return_quantiles(all_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Target\n",
    "We'll use shifted 5 day returns for training the model.\n",
    "\n",
    "As some alpha factors measure time is not same, we can also shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_factors which is no need to train\n",
    "universe = all_factors.copy(deep=True)\n",
    "all_factors = all_factors.replace([np.inf, -np.inf], np.nan)\n",
    "all_factors['target'] = all_factors.groupby('ts_code')['return_2q'].shift(-2).fillna(0).astype(np.int16)\n",
    "#all_factors['target'] = all_factors.groupby('ts_code')['return_2q'].shift(-2)\n",
    "\n",
    "all_factors = all_factors.dropna()\n",
    "all_factors['target'] = all_factors['target'].astype(np.int16)\n",
    "\n",
    "universe = universe.loc[universe['trade_date']<20230317]\n",
    "all_factors = all_factors.loc[all_factors['trade_date']<20230317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IID Check of Target\n",
    "Let's see if the returns are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.reset_index(inplace=True)\n",
    "all_factors.set_index(['date','ts_code'],inplace=True)\n",
    "#tmp = all_factors.loc[all_factors['trade_date']>20220601]\n",
    "ph.IID_check(all_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Valid/Test Splits\n",
    "Split the data into a train, validation, and test set. For this, we'll use some of the features and the 5 day returns for our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'total_mv' not add\n",
    "features = ['turnover_rate', 'atr_5', 'pe', 'pb', 'revenue_ps', 'total_mv',\n",
    "       'dt_eps_yoy', 'bps_yoy', 'roe_yoy', 'ebt_yoy', 'or_yoy',\n",
    "       'alpha_cci', 'alpha_supertrend', 'alpha_kama', 'alpha_close2open_5_sma',\n",
    "       'alpha_close2open_25_sma', 'alpha_skew2sentiment', 'alpha_fundamental',\n",
    "       'alpha_winlos'\n",
    "           ]+ sector_columns\n",
    "\n",
    "target_label = 'target'\n",
    "print(all_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_factors = all_factors.replace([np.inf, -np.inf], np.nan)\n",
    "#all_factors = all_factors[~all_factors.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "#all_factors.shape\n",
    "all_factors.reset_index(inplace=True)\n",
    "all_factors['date'] = pd.to_datetime(all_factors['trade_date'], format='%Y%m%d')\n",
    "all_factors = all_factors.set_index(['date','ts_code']).sort_values(by=['date'])\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = all_factors.dropna().copy()\n",
    "print(tmp.shape)\n",
    "X = tmp[features]\n",
    "y = tmp[target_label].astype(np.int16)\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = ph.train_valid_test_split(X, y, 0.6, 0.2, 0.2)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forests with Different Tree Sizes\n",
    "Let's build models using different tree sizes to find the model that best generalizes.\n",
    "#### Parameters\n",
    "When building the models, we'll use the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 20\n",
    "n_stocks = len(all_factors.index.get_level_values(1).unique())\n",
    "\n",
    "clf_parameters = {\n",
    "    'max_features':'sqrt',\n",
    "    'criterion': 'entropy',\n",
    "    #'min_samples_split' : 2500,\n",
    "    'min_samples_leaf': n_stocks*n_days,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 0}\n",
    "n_trees_l = [50, 150, 250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Important Features by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "feature_importances = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf.score(X_train, y_train.values))\n",
    "    valid_score.append(clf.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf.oob_score_)\n",
    "    feature_importances.append(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.plot(\n",
    "    [n_trees_l]*4,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')\n",
    "print('Features Ranked by Average Importance:\\n')\n",
    "ph.rank_features_by_importance(np.average(feature_importances, axis=0), features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an ensemble of non-overlapping trees\n",
    "The last method is to create ensemble of non-overlapping trees. Here we are going to write a custom `scikit-learn` estimator. We inherit from `VotingClassifier` and we override the `fit` method so we fit on non-overlapping periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "class NoOverlapVoter(VotingClassifier):\n",
    "    \n",
    "    def _calculate_oob_score(self, classifiers):\n",
    "        oob = 0\n",
    "        for clf in classifiers:\n",
    "            oob = oob + clf.oob_score_\n",
    "        return oob / len(classifiers)\n",
    "        \n",
    "    def _non_overlapping_estimators(self, x, y, classifiers, n_skip_samples):\n",
    "        estimators_ = []\n",
    "        for i in range(n_skip_samples):\n",
    "            estimators_.append(\n",
    "                classifiers[i].fit(x[i::n_skip_samples], y[i::n_skip_samples])\n",
    "            )\n",
    "        return estimators_\n",
    "    \n",
    "    def __init__(self, estimator, voting='soft', n_skip_samples=4):\n",
    "        # List of estimators for all the subsets of data\n",
    "        estimators = [('clf'+str(i), estimator) for i in range(n_skip_samples + 1)]\n",
    "        \n",
    "        self.n_skip_samples = n_skip_samples\n",
    "        super().__init__(estimators, voting=voting)\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        estimator_names, clfs = zip(*self.estimators)\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        \n",
    "        clone_clfs = [clone(clf) for clf in clfs]\n",
    "        self.estimators_ = self._non_overlapping_estimators(X, y, clone_clfs, self.n_skip_samples)\n",
    "        self.named_estimators_ = Bunch(**dict(zip(estimator_names, self.estimators_)))\n",
    "        self.oob_score_ = self._calculate_oob_score(self.estimators_)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 10\n",
    "n_stocks = len(all_factors.index.get_level_values(1).unique())\n",
    "\n",
    "clf_parameters = {\n",
    "    'max_features':'sqrt',\n",
    "    'criterion': 'entropy',\n",
    "    #'min_samples_split' : 800,\n",
    "    'min_samples_leaf': n_stocks*n_days,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 0}\n",
    "\n",
    "n_trees_l = [20, 25, 50, 100]\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    \n",
    "    clf_nov = NoOverlapVoter(clf)\n",
    "    clf_nov.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf_nov.score(X_train, y_train.values))\n",
    "    valid_score.append(clf_nov.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf_nov.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.plot(\n",
    "    [n_trees_l]*5,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Predict on the test data\n",
    "predictions = clf_nov.predict(X_valid)\n",
    "\n",
    "# Score our model\n",
    "print('Accuracy score: ', format(accuracy_score(y_valid.values, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_valid.values, predictions, average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_valid.values, predictions, average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_valid.values, predictions, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_names = [\n",
    "    'alpha_fundamental',\n",
    "    'turnover_rate',\n",
    "    'alpha_cci',\n",
    "    'alpha_kama',\n",
    "    'alpha_supertrend',\n",
    "    'alpha_skew2sentiment',\n",
    "   'alpha_winlos',\n",
    "    'atr_5'\n",
    "]\n",
    "\n",
    "all_pricing = all_factors.reset_index().pivot(index='date', columns='ts_code', values='close')\n",
    "all_pricing.head()\n",
    "\n",
    "tmp = all_factors.copy(deep=True)\n",
    "tmp['close'] = all_factors.groupby('ts_code')['close'].shift(-2)\n",
    "tmp.loc[tmp.index.get_level_values(1) == '603538.SH']['close']\n",
    "all_pricing = tmp.reset_index().pivot(index='date', columns='ts_code', values='close')\n",
    "all_pricing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalueate alpha factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "def show_sample_results(data, samples, classifier, factors, pricing=all_pricing):\n",
    "    # Calculate the Alpha Score\n",
    "    prob_array=[-1, 0, 1]\n",
    "    alpha_score = classifier.predict_proba(samples).dot(np.array(prob_array))\n",
    "    \n",
    "    # Add Alpha Score to rest of the factors\n",
    "    alpha_score_label = 'alpha_AI'\n",
    "    factors_with_alpha = data.loc[samples.index].copy()\n",
    "    factors_with_alpha[alpha_score_label] = alpha_score\n",
    "    \n",
    "    # Setup data for AlphaLens\n",
    "    print('Cleaning Data...\\n')\n",
    "    factor_data = ph.build_factor_data(factors_with_alpha[factors + [alpha_score_label]], pricing)\n",
    "    print('\\n-----------------------\\n')\n",
    "    \n",
    "    # Calculate Factor Returns and Sharpe Ratio\n",
    "    factor_returns = ph.get_factor_returns(factor_data)\n",
    "    sharpe_ratio = ph.sharpe_ratio(factor_returns)\n",
    "    \n",
    "    # Show Results\n",
    "    print('             Sharpe Ratios')\n",
    "    print(sharpe_ratio.round(2))\n",
    "    ph.plot_factor_returns(factor_returns)\n",
    "    ph.plot_factor_rank_autocorrelation(factor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_train, clf_nov, factor_names, pricing=all_pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf_nov, factor_names, pricing=all_pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "### Re-Training Model\n",
    "In production, we would roll forward the training. Typically you would re-train up to the \"current day\" and then test. Here, we will train on the train & validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 10\n",
    "n_stocks = len(all_factors.index.get_level_values(1).unique())\n",
    "\n",
    "clf_parameters = {\n",
    "    'max_features':'sqrt',\n",
    "    'criterion': 'entropy',\n",
    "    #'min_samples_split' : 1000,\n",
    "    'min_samples_leaf': n_stocks*n_days,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 0}\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "\n",
    "clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    \n",
    "clf_nov = NoOverlapVoter(clf)\n",
    "clf_nov.fit(\n",
    "    pd.concat([X_train, X_valid]),\n",
    "    pd.concat([y_train, y_valid]))\n",
    "\n",
    "train_score.append(clf_nov.score(X_train, y_train.values))\n",
    "valid_score.append(clf_nov.score(X_valid, y_valid.values))\n",
    "oob_score.append(clf_nov.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score)\n",
    "print(valid_score, oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Train Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, pd.concat([X_train, X_valid]), clf_nov, factor_names, pricing=all_pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Test Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([X_train, X_valid])\n",
    "tmp = pd.concat([tmp, X_test])\n",
    "show_sample_results(all_factors, X_test, clf_nov, factor_names, pricing=all_pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merage AI factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift data\n",
    "#all_factors = universe.copy(deep=True)\n",
    "# predict 2days\n",
    "#all_factors['alpha_cci'] = all_factors.groupby('ts_code')['alpha_cci'].shift(2)\n",
    "#all_factors['alpha_fundamental'] = all_factors.groupby('ts_code')['alpha_fundamental'].shift(5)\n",
    "#all_factors = all_factors.fillna(method='bfill')\n",
    "all_factors = all_factors.reset_index().set_index(['date', 'ts_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_AI_factors(data, classifier):\n",
    "    # Calculate the Alpha Score\n",
    "    samples = data[features]\n",
    "    prob_array=[-1,0,1]\n",
    "    alpha_score = classifier.predict_proba(samples).dot(np.array(prob_array))\n",
    "\n",
    "    # Add Alpha Score to rest of the factors\n",
    "    alpha_score_label = 'alpha_AI'\n",
    "    print(alpha_score, len(alpha_score))\n",
    "    factors_with_alpha = data.loc[samples.index].copy()\n",
    "    factors_with_alpha[alpha_score_label] = alpha_score\n",
    "    return factors_with_alpha\n",
    "    \n",
    "all_factors = save_AI_factors(all_factors, clf_nov)\n",
    "all_factors = all_factors.reset_index().set_index(['date'])\n",
    "all_factors = universe.merge(all_factors[['alpha_AI','ts_code','trade_date']], on=['ts_code','trade_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.loc[all_factors.alpha_AI.isnull()==False]['alpha_AI'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = ['ts_code', 'trade_date', 'name', 'industry', 'close', 'log-ret', 'return_2q', \n",
    "       'alpha_close2open', 'alpha_close2open_5_sma', 'alpha_close2open_25_sma', \n",
    "       'alpha_supertrend', 'alpha_cci', 'alpha_kama', 'alpha_skew2sentiment',\n",
    "       'alpha_fundamental','alpha_AI']\n",
    "all_factors[field].to_csv('factors_AI_20170405_20230317.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = all_factors[field]\n",
    "#tmp = tmp.loc[tmp.industry=='化学制药']\n",
    "tmp = tmp.loc[tmp['trade_date']>20220501]\n",
    "tmp.to_csv('factor_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
