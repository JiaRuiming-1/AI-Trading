{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "In this notebook, we will complete backtest which is the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from statistics import median\n",
    "from scipy.stats import gaussian_kde, zscore\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Factors\n",
    "We have factors store in csv file which process and combine from privious steps. Pick up backtest time from 2022.4 - 2022.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_raw = feather.read_dataframe('AI_data/factors_AI_bitcon.feather')\n",
    "#universe_raw['date'] = pd.to_datetime(universe_raw['trade_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "universe_raw['date'] = universe_raw['trade_date'].apply(lambda x: pd.Timestamp(x))\n",
    "universe = universe_raw.set_index(['date']).sort_values(by=['date', 'ts_code'])\n",
    "universe = universe.fillna(method='ffill').fillna(0.)\n",
    "print(universe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use from 2022.6 to 2023.4, here from 2022.5 in order to generate risk model from history\n",
    "universe = universe.loc[(universe.index>='2022-03-01 00:00:00') & (universe.index<='2023-03-28 00:00:00')]\n",
    "universe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['trade_date'] = universe.index\n",
    "universe.loc[universe.index==universe.index.unique()[-1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.trade_date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.ts_code.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale AI Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rescale_(data):\n",
    "    # resize from -1 to 1\n",
    "    feature = 'alpha_AI'\n",
    "    data[feature] = data[feature].astype(np.float32)\n",
    "    data[[feature]] = data[[feature]].apply(zscore)\n",
    "    max_val = data[feature].median() + 3.3*data[feature].std()\n",
    "    min_val = data[feature].median() - 3.3*data[feature].std()\n",
    "    data[feature] = np.where(data[feature]>max_val, max_val, \n",
    "                           np.where(data[feature]<min_val, min_val, data[feature]))\n",
    "    return data\n",
    "\n",
    "\n",
    "universe = universe.groupby('trade_date').apply(rescale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_field = [\n",
    "       'alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI', \n",
    "        ] \n",
    "universe[alpha_field] = universe[alpha_field]/3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_tickers(df):\n",
    "    ticker_list = df.ts_code.unique()\n",
    "    calendar = df.index.unique()\n",
    "    for dt in tqdm(calendar):\n",
    "        day_df = df.loc[df.index == dt]\n",
    "        diff_tickers = list(set(ticker_list) - set(day_df.ts_code.unique()))\n",
    "        if len(diff_tickers) > 0:\n",
    "            for ticker in diff_tickers:\n",
    "                check_df = df.loc[df.ts_code==ticker].iloc[0,:]\n",
    "                #'ts_code', 'trade_date', 'close', 'log-ret', \n",
    "                # 'alpha_019', 'alpha_022', 'alpha_078', 'alpha_AI'\n",
    "                data = [pd.Timestamp(dt), ticker, dt, 0., 0., 0., 0., 0., 0.]\n",
    "                data = pd.DataFrame(data=data, index=np.append(['date'], df.columns)).T.set_index(['date'])\n",
    "                df = df.append(data)\n",
    "    df = df.sort_values(by=['date', 'ts_code']) \n",
    "    return df\n",
    "\n",
    "universe = alignment_tickers(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universe['alpha_AI2'] = (universe['alpha_AI'] + universe['alpha_078'])/2\n",
    "#print(universe['alpha_AI2'].median())\n",
    "universe['alpha_028'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.ts_code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['close'] = universe['close'].replace(0, np.nan)\n",
    "universe['pct_chg'] = universe.groupby('ts_code')['close'].pct_change().fillna(0.)\n",
    "#tmp = universe.loc[universe.ts_code=='BCHUSDT']\n",
    "#tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Exposures and Factor Returns\n",
    "The facort values in cross section should view as a type of exposure. We can calculate factor returns bettwen exposures of each ticker and daily return. We also did this in backtestig animate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shif return 5 days\n",
    "all_factors = universe.copy(deep=True)\n",
    "all_factors = all_factors.sort_values(by=['date', 'ts_code'])\n",
    "def return_handle(df):\n",
    "    df['returns_2'] = df['pct_chg'].shift(-1)\n",
    "    return df\n",
    "all_factors = all_factors.groupby('ts_code').apply(return_handle)\n",
    "all_factors = all_factors.replace([np.inf, -np.inf], np.nan).fillna(0.).sort_values(by=['date', 'ts_code'])\n",
    "print(universe.shape, all_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = all_factors.loc[all_factors.index >= '2022-04-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    return np.where(x <= a,a, np.where(x >= b, b, x))\n",
    "\n",
    "def get_formula(factors, Y):\n",
    "    L = [\"0\"]\n",
    "    L.extend(factors)\n",
    "    return Y + \" ~ \" + \" + \".join(L)\n",
    "\n",
    "def factors_from_names(n, name):\n",
    "    return list(filter(lambda x: name in x, n))\n",
    "\n",
    "def estimate_factor_returns(df, name='alpha_'): \n",
    "    ## winsorize returns for fitting \n",
    "    estu = df.copy(deep=True)\n",
    "    estu['returns_2'] = wins(estu['returns_2'], -0.5, 0.5)\n",
    "    all_factors = factors_from_names(list(df), name)\n",
    "    results = pd.Series()\n",
    "    for factor_name in all_factors:\n",
    "        form = get_formula([factor_name], \"returns_2\")\n",
    "        model = ols(form, data=estu)\n",
    "        result = model.fit()\n",
    "        results = results.append(result.params)\n",
    "    return results\n",
    "\n",
    "estimate_factor_returns(all_factors.loc[all_factors['trade_date']=='2022-07-01 00:00:00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_field = ['ts_code', 'trade_date', 'log-ret', 'close']\n",
    "date_and_code = [ 'ts_code', 'returns_2']\n",
    "\n",
    "start_time = '2022-04-28'\n",
    "alpha_df = all_factors[alpha_field + date_and_code].copy(deep=True)\n",
    "alpha_df = alpha_df.loc[alpha_df.index>=start_time].sort_values(by=['date', 'ts_code'])\n",
    "calendar = alpha_df.index.unique() # int64\n",
    "\n",
    "#only for positive estimate\n",
    "# for feature in alpha_field:\n",
    "#     alpha_df[feature] = np.where(alpha_df[feature]>=0.7, alpha_df[feature], np.where(alpha_df[feature]<=-0.7, alpha_df[feature], 0))\n",
    "#     alpha_df[feature] = np.where(alpha_df[feature]>0, alpha_df[feature], 0.)\n",
    "\n",
    "facret = {}\n",
    "for dt in tqdm(calendar, desc='factor returns regression'):\n",
    "    facret[dt] = estimate_factor_returns(alpha_df.loc[alpha_df.index==dt])\n",
    "facret[calendar[-5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veiw Factor Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = alpha_df.index.unique()\n",
    "facret_df = pd.DataFrame(index = date_list)\n",
    "\n",
    "for ii, dt in zip(calendar,date_list): \n",
    "    for alp in alpha_field: \n",
    "        facret_df.at[dt, alp] = facret[ii][alp]\n",
    "\n",
    "for column in ['alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI']:\n",
    "    plt.plot(facret_df[column].cumsum(), label=column)\n",
    "    #plt.plot((1+facret_df[column]).cumprod(), label=column)\n",
    "    \n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Factor Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA model\n",
    "We use PCA algorithm to estimate risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class RiskModel(object):\n",
    "    def __init__(self, returns, num_factor_exposures, ann_factor=96):\n",
    "        \n",
    "        self.num_factor_exposures = num_factor_exposures\n",
    "        self.pca = PCA(n_components=num_factor_exposures, svd_solver='full')\n",
    "        self.pca.fit(returns)\n",
    "        \n",
    "        self.factor_betas_ = self.factor_betas(self.pca, returns.columns.values, np.arange(num_factor_exposures))\n",
    "        self.factor_returns_ = self.factor_returns(self.pca, returns, returns.index, np.arange(num_factor_exposures))\n",
    "        self.estimate_returns = self.estimate_returns(self.factor_returns_, self.factor_betas_, returns)\n",
    "        self.factor_cov_matrix_ = self.factor_cov_matrix(self.factor_returns_, ann_factor)\n",
    "        \n",
    "        self.idiosyncratic_var_matrix_ = self.idiosyncratic_var_matrix(returns, self.estimate_returns, ann_factor)\n",
    "        self.idiosyncratic_var_vector = pd.DataFrame(data=np.diag(self.idiosyncratic_var_matrix_),\n",
    "                                                     index=returns.columns)\n",
    "    \n",
    "    # got new exposure expressed by pca model\n",
    "    def factor_betas(self, pca, factor_beta_indices, factor_beta_columns):\n",
    "        return pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\n",
    "    \n",
    "    # got new factor returns expressed by pca model\n",
    "    def factor_returns(self, pca, returns, factor_return_indices, factor_return_columns):\n",
    "        return pd.DataFrame(pca.transform(returns), factor_return_indices, factor_return_columns)\n",
    "    \n",
    "    # got new factor covariance matirx by pca expressed returns\n",
    "    def factor_cov_matrix(self, factor_returns, ann_factor):\n",
    "        return np.diag(factor_returns.var(axis=0, ddof=1) * ann_factor)\n",
    "    \n",
    "    def estimate_returns(self, factor_returns, factor_betas, returns):\n",
    "        return pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n",
    "    \n",
    "    # calculate idiosyncratic need to got factor_returns, factor_betas which calculate by pca model first\n",
    "    def idiosyncratic_var_matrix(self, returns, estimate_returns, ann_factor):\n",
    "        residuals = returns - estimate_returns\n",
    "        return pd.DataFrame(np.diag(np.var(residuals))*ann_factor, returns.columns, returns.columns)\n",
    "    \n",
    "    def plot_principle_risk(self):\n",
    "        # Make the bar plot\n",
    "        plt.bar(np.arange(self.num_factor_exposures), self.pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_df_by_day(df, start_time):\n",
    "    pca_time_window = len(df.loc[df['trade_date']<start_time].trade_date.unique())\n",
    "    print(f'pca window_len is {pca_time_window}')\n",
    "    #trade_date_list = df.loc[df['trade_date']>=start_time].trade_date.unique()\n",
    "    all_date_list = df.trade_date.unique()\n",
    "    for start_i in range(len(all_date_list)):\n",
    "        start_date = all_date_list[start_i]\n",
    "        if start_i + pca_time_window >= len(all_date_list):\n",
    "            break\n",
    "        end_date = all_date_list[start_i + pca_time_window]\n",
    "        yield end_date, df.loc[(df['trade_date']>=start_date) & (df['trade_date']<=end_date)]\n",
    "        \n",
    "def risk_by_PCA(returns_df):\n",
    "#     for col in returns_df.columns:\n",
    "#         returns_df[col] = np.where(returns_df[col]<0, returns_df[col], 0.)\n",
    "    \n",
    "    # Set the number of factor exposures (principal components) for the PCA algorithm\n",
    "    num_factor_exposures = 5\n",
    "    # Create a RiskModel object\n",
    "    rm = RiskModel(returns_df, num_factor_exposures)\n",
    "    \n",
    "    B = rm.factor_betas_\n",
    "    F = rm.factor_cov_matrix_\n",
    "    S = rm.idiosyncratic_var_matrix_\n",
    "    fr = rm.factor_returns_\n",
    "    #fr = rm.estimate_returns\n",
    "    \n",
    "    variance = np.dot(B, F).dot(B.T) + S\n",
    "    return variance, B, fr, rm.idiosyncratic_var_vector, F\n",
    "\n",
    "# test\n",
    "dt, df = next(rolling_df_by_day(all_factors, start_time))\n",
    "returns_df = df.pivot(index='trade_date', columns='ts_code', values='log-ret').fillna(0)\n",
    "variance_i, B, risk_fr, residual_i, F = risk_by_PCA(returns_df)\n",
    "print(f'return date {dt}')\n",
    "variance_i.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.array([1/36]*36) \n",
    "(np.dot(h, variance_i).dot(h.T))** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_all = {}\n",
    "residual_df = pd.DataFrame()\n",
    "\n",
    "for dt, df in rolling_df_by_day(all_factors, start_time):\n",
    "    ticker_list = all_factors.loc[all_factors.trade_date==dt].ts_code.unique()\n",
    "    df = df.loc[df.ts_code.isin(ticker_list)]\n",
    "    returns_df = df.pivot(index='trade_date', columns='ts_code', values='log-ret').fillna(0)\n",
    "    variance_i, B, risk_factor, residual_i, F = risk_by_PCA(returns_df)\n",
    "    variance_all[dt] = [variance_i, B, risk_factor.iloc[-1,:], residual_i.copy(), F]\n",
    "    residual_i['trade_date'] = df.loc[df.index[-1],'trade_date'].unique()[-1]\n",
    "    residual_df = residual_df.append(residual_i)\n",
    "\n",
    "residual_df.reset_index(inplace=True)\n",
    "residual_df.columns = ['ts_code', 'residual', 'trade_date']\n",
    "residual_df['residual'] = np.where(residual_df['residual'].isnull(), residual_df['residual'].median(), residual_df['residual'])\n",
    "all_factors = all_factors.loc[all_factors['trade_date']>=start_time]\n",
    "all_factors = all_factors.merge(residual_df, on=['trade_date','ts_code'], how='left')\n",
    "#all_factors.tail()\n",
    "print(residual_df.shape, all_factors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI', \n",
    "#all_factors['date'] = pd.to_datetime(all_factors['trade_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "all_factors['date'] = all_factors['trade_date'].apply(lambda x: pd.Timestamp(x))\n",
    "alpha_df = all_factors.drop(columns=['returns_2']).set_index(['date', 'ts_code']).sort_index(level=['date', 'ts_code'])\n",
    "#alpha_df['alpha_all'] = 0.5 * alpha_df['alpha_AI'] + 0.5 * alpha_df['alpha_022']\n",
    "alpha_df['alpha_all'] = alpha_df['alpha_028']\n",
    "print(max(alpha_df['alpha_all']), min(alpha_df['alpha_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dt in alpha_df.index.get_level_values(0).unique():\n",
    "#     tmp = alpha_df.loc[dt]\n",
    "#     print(tmp['trade_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(calendar[0]), type(alpha_df.index.get_level_values(0).unique()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_df.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest by Convex Optimization(Choice one option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option One\n",
    "- use `cvxpy` to optimized portfolio\n",
    "- lambda expressed a transaction costs weights\n",
    "- aversion expressed max risk to endure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = alpha_df.trade_date.unique()\n",
    "ticker_num = len(alpha_df.index.get_level_values(1).unique())\n",
    "\n",
    "# get parameter\n",
    "positions = {}\n",
    "alpha_df['h_privious'] = 0.\n",
    "h0 = [0.] * ticker_num\n",
    "aversion = 0.1\n",
    "Lambda = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_df.loc[alpha_df.index.get_level_values(0)==calendar[-2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.project_helper import OptimalHoldings\n",
    "\n",
    "#dt = '2022-04-10 00:00:00'\n",
    "dt = calendar[100]\n",
    "obj_df = alpha_df.loc[alpha_df.trade_date=='2022-07-10 00:00:00']\n",
    "alpha_vector = obj_df.loc[obj_df.index.get_level_values(0)[-1]][['alpha_all']]\n",
    "optimal_weights = OptimalHoldings(aversion=.1, weights_max=0.25, weights_min=-0.25, lambda_reg=.3).find(alpha_vector, variance_all[dt][1],\n",
    "                            variance_all[dt][4],  variance_all[dt][3], obj_df['h_privious'], Lambda = Lambda ** 2)\n",
    "optimal_weights.loc[optimal_weights[0]<=-0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_obj = OptimalHoldings(aversion=.1, weights_max=0.25, weights_min=-0.25, lambda_reg=0.3)\n",
    "for dt in tqdm(calendar, desc='optimized holding...'):\n",
    "    # fill yesterday holding\n",
    "    obj_df = alpha_df.loc[alpha_df.trade_date==dt]\n",
    "    alpha_vector = obj_df.loc[obj_df.index.get_level_values(0)[-1]][['alpha_all']]\n",
    "    # convex optimize\n",
    "    optimal_weights = oh_obj.find(alpha_vector, variance_all[dt][1],\n",
    "                            variance_all[dt][4],  variance_all[dt][3], obj_df['h_privious'],Lambda = 1e-8)\n",
    "    h_optimal = optimal_weights\n",
    "    # update optimize holding\n",
    "    obj_df['h_opt'] = h_optimal.values\n",
    "    obj_df['h_privious'] = h0\n",
    "    positions[dt]= obj_df\n",
    "    h0 = h_optimal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option Two\n",
    "- use `scipy.optimize.fmin_l_bfgs_b` to optimized portfolio\n",
    "- lambda expressed a transaction costs weights\n",
    "- aversion expressed a risk 1 posistion map to reduce how much risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lambda expressed a transaction costs weights\n",
    "def get_obj_func(h0, variance, alpha, Lambda=3e-6, aversion=1e-4): \n",
    "    def obj_func(h):\n",
    "        #f = 0.5 * aversion * np.dot(h, variance).dot(h.T) - aversion * np.matmul(h, alpha) + np.dot((h-h0)**2, Lambda)\n",
    "        f =  - np.matmul(h, alpha) \\\n",
    "             + 0.5 * aversion * (np.dot(h, variance).dot(h.T)) \\\n",
    "             + np.dot((h-h0)**2, Lambda) \\\n",
    "             + 0.5 * alpha * np.linalg.norm(h) \\\n",
    "\n",
    "             \n",
    "        return f\n",
    "    return obj_func\n",
    "\n",
    "def get_grad_func(h0, variance, alpha, Lambda=3e-6, aversion=1e-4):\n",
    "    def grad_func(h):\n",
    "        #f_hat =  aversion * np.dot(variance, h) - aversion * alpha + 2 * Lambda * (h-h0)\n",
    "        f_hat =  - alpha  \\\n",
    "                + aversion * np.dot(variance, h) \\\n",
    "                + 2 * Lambda * (h-h0) \\\n",
    "                + 0.5 * 0.5 * alpha\n",
    "        return f_hat\n",
    "    return grad_func\n",
    "\n",
    "# get parameter\n",
    "bounds = [(0, 10000)] * ticker_num\n",
    "\n",
    "# for dt in calendar:\n",
    "#     # fill yesterday holding\n",
    "#     obj_df = alpha_df.loc[alpha_df.trade_date==dt]\n",
    "    \n",
    "#     # convex optimize\n",
    "#     obj_func = get_obj_func(h0, variance_all[dt][0], obj_df['alpha_all'].values)\n",
    "#     grad_func = get_grad_func(h0, variance_all[dt][0], obj_df['alpha_all'].values)\n",
    "#     h_optimal, min_val, _ = fmin_l_bfgs_b(obj_func, h0, fprime=grad_func, bounds=bounds)\n",
    "#     #h_optimal, min_val, _ = fmin_l_bfgs_b(obj_func, h0, fprime=grad_func)\n",
    "#     # update optimize holding\n",
    "#     obj_df['h_opt'] = h_optimal\n",
    "#     obj_df['h_privious'] = h0\n",
    "#     positions[dt]= obj_df\n",
    "#     h0 = h_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "positions_bak = copy.deepcopy(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = copy.deepcopy(positions_bak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = []\n",
    "for dt in list(positions.keys()):\n",
    "    all_positions.append(positions[dt].h_opt.sum())\n",
    "print(max(all_positions), min(all_positions))\n",
    "print(np.mean(all_positions), np.median(all_positions))\n",
    "#positions[20230315]['h_opt'].hist()\n",
    "calendar = all_factors['trade_date'].unique()\n",
    "pd.Series(all_positions, index=pd.to_datetime(calendar, format='%Y-%m-%d %H:%M:%S')).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Money to optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_money = 1e5\n",
    "for i in positions.keys():\n",
    "    positions[i]['h_opt'] = positions[i]['h_opt'] * holding_money\n",
    "    positions[i]['h_privious'] = positions[i]['h_privious'] * holding_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_optimal_list = [positions[dt]['h_opt'] for dt in positions.keys() ]\n",
    "h_privious_list = [positions[dt]['h_privious'] for dt in positions.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = []\n",
    "for dt in list(positions.keys()):\n",
    "    all_positions.append(positions[dt].h_opt.sum())\n",
    "print(max(all_positions), min(all_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_hold = list(positions.keys())[-1]\n",
    "positions[date_hold]['h_opt'].hist()\n",
    "tmp = positions[date_hold].loc[(positions[date_hold]['h_opt']>1000) | (positions[date_hold]['h_opt']<-1000)]\n",
    "print(positions[date_hold].loc[positions[date_hold]['h_opt']>0]['h_opt'].sum(), tmp.shape)\n",
    "tmp[['close','log-ret','h_privious', 'h_opt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Exposure and Transaction Costs\n",
    "We use pca to calculate risk, so we can view residual(alpha) as risk exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = [7, 3]\n",
    "risk_exposures_df = pd.DataFrame()\n",
    "risk_exposures = {}\n",
    "for ii, dt in enumerate(positions.keys()):\n",
    "    # estimate_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n",
    "    B = variance_all[dt][1]\n",
    "    h_opt_i = h_optimal_list[ii]\n",
    "    risk_exposure = np.matmul(h_opt_i.T, B)\n",
    "    risk_exposures[dt] = risk_exposure\n",
    "    risk_exposures_df = risk_exposures_df.append(risk_exposure, ignore_index=True)\n",
    "\n",
    "#np.sum(risk_exposures)\n",
    "risk_exposures_df.set_index(pd.to_datetime(calendar, format='%Y-%m-%d %H:%M:%S'), inplace=True)\n",
    "risk_exposures_df.plot(grid=True, title='Risk Exposure')\n",
    "#risk_exposures_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import pandas\n",
    "\n",
    "def colnames(B):\n",
    "    if type(B) == patsy.design_info.DesignMatrix: \n",
    "        return B.design_info.column_names\n",
    "    if type(B) == pandas.core.frame.DataFrame: \n",
    "        return B.columns.tolist()\n",
    "    return None\n",
    "\n",
    "def get_B_alpha(universe):\n",
    "    alpha_factors = factors_from_names(list(universe),'alpha_')\n",
    "    formula = get_formula(alpha_factors, \"returns_2\")\n",
    "    outcome, B_alpha = patsy.dmatrices(formula, universe)\n",
    "    return B_alpha\n",
    "\n",
    "alpha_df_ = alpha_df.merge(all_factors[['trade_date','ts_code','returns_2']], on=['trade_date','ts_code'], how='left')\n",
    "calendar = alpha_df.trade_date.unique()\n",
    "alpha_exposure_df = pd.DataFrame()\n",
    "alpha_exposures = {}\n",
    "for ii, dt in enumerate(calendar):\n",
    "    alpha_df_i = alpha_df_.loc[alpha_df_.trade_date==dt][['trade_date','ts_code',\n",
    "                            'alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI',  'returns_2']]\n",
    "    h_opt_i = h_privious_list[ii]\n",
    "    B_alpha = get_B_alpha(alpha_df_i)\n",
    "    \n",
    "    B_alpha = B_alpha * [[1., 0., 0., 0.]] \n",
    "    #alpha_exposure = pd.Series(np.matmul(B_alpha.transpose(), h_opt_i), index=colnames(B_alpha))\n",
    "    alpha_exposure = pd.Series(np.matmul(B_alpha.transpose(), h_opt_i), index=[\n",
    "                    'alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI', ])\n",
    "    alpha_exposures[dt] = alpha_exposure\n",
    "    alpha_exposure_df = pd.concat([alpha_exposure_df,alpha_exposure], axis=1)\n",
    "\n",
    "alpha_exposure_df = alpha_exposure_df.T.reset_index(drop=True)\n",
    "alpha_exposure_df.set_index(pd.to_datetime(calendar, format='%Y-%m-%d %H:%M:%S'), inplace=True)\n",
    "alpha_exposure_df.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retre_list = []\n",
    "for i in range(1,len(h_optimal_list)):\n",
    "    tmp_change = h_optimal_list[i] - h_privious_list[i]\n",
    "    retre_list.append(tmp_change.abs().max())\n",
    "print(np.sum(retre_list)/4177, np.max(retre_list))\n",
    "pd.Series(retre_list, index=pd.to_datetime(calendar[1:], format='%Y%m%d %H:%M:%S')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_costs = []\n",
    "Lambda = 5e-8\n",
    "for i in range(1,len(h_optimal_list)):\n",
    "    tmp_change = h_optimal_list[i] - h_privious_list[i]\n",
    "    costs = sum(np.dot(tmp_change**2, Lambda)**0.5)\n",
    "    transaction_costs.append(costs)\n",
    "print(np.sum(transaction_costs))\n",
    "pd.Series(transaction_costs, index=pd.to_datetime(calendar[1:], format='%Y%m%d %H:%M:%S')).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cal BenchMark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_return = pd.read_csv('AI_data/benmark_AI_119.csv')\n",
    "benchmark_return.columns = ['trade_date', 'log-ret']\n",
    "benchmark_return.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit-and-Loss (PnL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['date'] = pd.to_datetime(all_factors['trade_date'], format='%Y%m%d %H:%M:%S')\n",
    "all_factors = all_factors.set_index(['date']).sort_values(by=['date'])\n",
    "\n",
    "## assumes v, w are pandas Series \n",
    "def partial_dot_product(v, w):\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution(): \n",
    "\n",
    "    df = pd.DataFrame(index = all_factors['trade_date'].unique()).iloc[:-2,:]\n",
    "    calendar = all_factors.trade_date.unique()\n",
    "    counter = range(len(calendar))\n",
    "    for ii, dt, time_i in zip(counter,calendar,df.index):\n",
    "        # holding frame\n",
    "        p = positions[dt]\n",
    "        # alpha f\n",
    "        #fr = facret[dt][[0,2,5]]\n",
    "        fr = facret[pd.Timestamp(dt)].loc[['alpha_028', 'alpha_ppo', 'alpha_wtr', 'alpha_AI']]\n",
    "        # risk f\n",
    "        rr = variance_all[dt][2]\n",
    "        row_universe = all_factors.loc[all_factors.trade_date==dt]\n",
    "        mf = p[['h_privious', 'h_opt']].merge(row_universe[['ts_code', 'returns_2']], how = 'left', on = \"ts_code\")  \n",
    "        mf['returns_2'] = wins(mf['returns_2'], -0.3, 0.3)\n",
    "        df.at[time_i,\"time.pnl\"] = np.sum(mf['h_opt'] * mf['returns_2'])\n",
    "        df.at[time_i,\"alpha.pnl\"] = partial_dot_product(fr, alpha_exposures[dt])\n",
    "        #df.at[time_i,\"risk.pnl\"] = partial_dot_product(rr, risk_exposures[dt])\n",
    "        #df.at[time_i,\"risk.pnl\"] = np.sum(rr.values * risk_exposures[dt].values)/(-1)\n",
    "        df.at[time_i,\"cost\"] = transaction_costs[ii]\n",
    "        #df.at[time_i,\"benchmark.pnl\"] = positions[dt]['h_opt'].sum() * \\\n",
    "                            #benchmark_return.loc[benchmark_return.trade_date==dt]['log-ret'].values[0]\n",
    "    \n",
    "    print(time_i)\n",
    "    return df\n",
    "\n",
    "attr = build_pnl_attribution()\n",
    "plt.grid(True)\n",
    "for column in attr.columns:\n",
    "    plt.plot(attr[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PnL Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总收益\n",
    "attr['time.pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小余额\n",
    "attr['time.pnl'].cumsum().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大余额时间\n",
    "attr['time.pnl'].cumsum().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因子归因收益\n",
    "attr['alpha.pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因子夏普比率\n",
    "(np.sqrt(252 * 12) * attr['alpha.pnl'].mean()- 0.00017*100000)/attr['alpha.pnl'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回测夏普比率\n",
    "(np.sqrt(252 * 12)* attr['time.pnl'].mean() - 0.00017*100000)/attr['time.pnl'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大回撤及时间\n",
    "portfolio_values = (100000 + attr['time.pnl'])\n",
    "retre = ((portfolio_values.cummax() - portfolio_values.cummin())/portfolio_values.cummax())\n",
    "max_retre_val = retre.max()\n",
    "max_retre_time = retre.idxmax()\n",
    "print(max_retre_time, max_retre_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 胜率\n",
    "attr.loc[attr['time.pnl']>0].shape[0]/attr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均盈利比率\n",
    "attr.loc[attr['time.pnl']>0]['time.pnl'].sum()/1000/attr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均每笔收益\n",
    "attr['time.pnl'].sum()/1000/attr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr['time.pnl'].cumsum().max()/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(variance_all[dt][2] * risk_exposures[dt].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr.loc[attr['time.pnl']>0].shape[0]/attr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_cap = pd.Series()\n",
    "close_cap = pd.Series()\n",
    "for i in range(1,len(h_optimal_list)):\n",
    "    tmp_change = h_optimal_list[i] - h_privious_list[i]\n",
    "    tmp_change = tmp_change[abs(tmp_change)>1000]\n",
    "    open_cap = open_cap.append(tmp_change[tmp_change>0])\n",
    "    close_cap = close_cap.append(tmp_change[tmp_change<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_cap.sum() - close_cap.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_cap.size + open_cap.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
