{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [4, 3]\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.normal(size=(50, 2))\n",
    "y = np.zeros(X.shape[0], dtype=np.int)\n",
    "y[X[:, 1] > 1.2] = 1\n",
    "# add one point\n",
    "X = np.vstack((X,[[0.5, 2.5], [2.1, 1.4], [1.9, 1.5]]))\n",
    "y = np.append(y, [0, 0, 0])\n",
    "def plot_points(X, y):\n",
    "    x_up = X[np.argwhere(y==1)]\n",
    "    x_low = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in x_up], [s[0][1] for s in x_up], s = 25, color = 'red', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in x_low], [s[0][1] for s in x_low], s = 25, color = 'cyan', edgecolor = 'k')\n",
    "    #plt.plot(X[:,0], [1.2]*len(y))\n",
    "    #plt.plot([1.716]*len(y), np.linspace(1.2,2.8, len(y)))\n",
    "    #plt.plot(X[:,0], [2.37]*len(y))\n",
    "    plt.xlabel('dim1')\n",
    "    plt.ylabel('dim2')\n",
    "    \n",
    "plot_points(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "tree = model.fit(X, y)\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "treedot = export_graphviz(\n",
    "        model,\n",
    "        out_file=None,\n",
    "        filled=True, rounded=True,\n",
    "        special_characters=True,\n",
    "        rotate=True\n",
    "    )\n",
    "treegraph = Source(treedot)\n",
    "treegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析步骤：\n",
    "依据不同标签数据的特征构建标签判别决策树模型，并对模型判别给出特征差异大小排序，从而定位特征差异结果。\n",
    "1. 数据：ATE_PASS 146个log多芯片。ATE_FAILED 600个log单芯片\n",
    "2. 清洗：过滤出所有具备测试值的测试项整理为Dateframe表格\n",
    "3. 过滤：整合清洗数据为单DateFrame，行为文件名，列为Pin+Number名，并构建对应PF标签（可暂存最后一列或新Dateframe）\n",
    "4. 建模：构建随机森林模型，并切分原数据为 Train，Valid，Test三部分\n",
    "5. 训练：训练模型，通过调整参数使Train,Valid趋近收敛精度达到80%以上（实际判别过程过于简单可合并train 与 valid 数据一起训练模型）\n",
    "6. 细分：根据分析结果抽样单独用例组合训练数据构建决策树模型给出当前用例下的特征差异性，基于此特征差异判断下计算判别成功率（90%以上）。\n",
    "7. 验证：基于夏普加性解释使用验证集或测试集数据对随机森林模型特征重要性进行二次判断验证特征重要性排序结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Prepare\n",
    "### load package\n",
    "I define a `project_helper` python file to wrap some function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "# custom py file\n",
    "import project_helper as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_good = '../log_good/'\n",
    "path_bad = '../log_bad_20230315/'\n",
    "path_bad2 = '../log_bad_20230313/'\n",
    "\n",
    "files_good = os.listdir(path_good)\n",
    "files_bad = os.listdir(path_bad)\n",
    "files_bad2 = os.listdir(path_bad2)\n",
    "\n",
    "files_list_good = [path_good + file for file in files_good if not os.path.isdir(file)]\n",
    "files_list_bad = [path_bad + file for file in files_bad if not os.path.isdir(file)]\n",
    "files_list_bad2 = [path_bad2 + file for file in files_bad2 if not os.path.isdir(file)]\n",
    "valid_file = 'SD8023EV101_OSDC_SITE1_negsample.txt'\n",
    "\n",
    "print(f'total goodchip files count is {len(files_list_good)}')\n",
    "print(f'total badchip files count is {len(files_list_bad)}')\n",
    "print(f'total badchip files count is {len(files_list_bad2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Clean\n",
    "### parse log data to one dataframe\n",
    "For fast run, advice load file list length blow 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all good data\n",
    "df_raw_good = ph.get_test_df((random.sample(files_list_good, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random extract part of bad data \n",
    "f_list = random.sample(files_list_bad, 50) +  random.sample(files_list_bad2, 50)\n",
    "df_raw_bad = ph.get_test_df(f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_unknown = ph.get_test_df([valid_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look of raw df\n",
    "df_raw_bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filte Data\n",
    "Pivote data from raw df to a new one. The new df row is filename and columns is Pins+P/F(at last) of all test in a log.\n",
    "#### Note:\n",
    "I fill nan to zero and change Measured to Measure/Force value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivote_df_good, case2pins = ph.concat_tests_pivot(df_raw_good, 1)\n",
    "pivote_df_bad, case2pins = ph.concat_tests_pivot(df_raw_bad, 0)\n",
    "\n",
    "pivote_df_good = pivote_df_good.fillna(0.)\n",
    "pivote_df_bad = pivote_df_bad.fillna(0.)\n",
    "\n",
    "# good and bad pin are different, fix it\n",
    "pivote_df_good = pivote_df_good[pivote_df_bad.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivote_df_bad.shape, pivote_df_good.shape)\n",
    "pivote_df_good.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model\n",
    "\n",
    "### split data\n",
    "split data into three sections involve train valid and test\n",
    "#### Note\n",
    "Due to negative data is much larger than positive, so the bad data can be extracted random samples which size as same as good data.\n",
    "\n",
    "Integrate good and bad sample data into one dataframe.\n",
    "\n",
    "Due to `train_data_split` function shuffle data rows. It need to be split into train valid and test data before seperate data into features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  extract samples from bad data\n",
    "pivote_df_bad_sample = pivote_df_bad.sample(frac=1.)\n",
    "print(f'Extract bad data samples shape: {pivote_df_bad_sample.shape}')\n",
    "\n",
    "# append good and bad data into one dataframe\n",
    "all_df = pivote_df_good.append(pivote_df_bad_sample)\n",
    "all_df.replace([np.inf, -np.inf], 0., inplace=True)\n",
    "print(f'Full data shape(good add bad samples): {all_df.shape}')\n",
    "\n",
    "# spilt all data into train valid and test \n",
    "df_train, df_valid, df_test = ph.train_data_split(all_df.copy().fillna(0.), set_size=[0.6, 0.4, 0.0])\n",
    "\n",
    "# seperate df into frature and target\n",
    "features_columns = [feature for feature in all_df.columns if feature != 'PF']\n",
    "y_train, y_valid, y_test = df_train['PF'], df_valid['PF'], df_test['PF']\n",
    "X_train, X_valid, X_test = df_train[features_columns], df_valid[features_columns], df_test[features_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Complete Model\n",
    "### random forests model\n",
    "\n",
    "train and valid model then check accuracy and tune up model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_parameters = {\n",
    "    # 'n_estimators': 200,\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth':3,\n",
    "    #'min_samples_split': int(pivote_df_bad_sample.shape[0] * 0.1),\n",
    "    'min_samples_leaf': int(pivote_df_bad_sample.shape[0] * 0.1),\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state' : 0,\n",
    "    'oob_score':True,\n",
    "}\n",
    "\n",
    "n_trees_l = [20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, ExtraTreesRegressor, RandomForestRegressor\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "feature_importances = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf.score(X_train, y_train.values))\n",
    "    valid_score.append(clf.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf.oob_score_)\n",
    "    feature_importances.append(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score)\n",
    "print(valid_score)\n",
    "print(oob_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Features Ranked by Average Importance:\\n')\n",
    "importance_order = ph.rank_features_by_importance(np.average(feature_importances, axis=0), features_columns, max_print=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.plot_tree_classifier(clf, tree_num=0, feature_names=features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.plot_tree_classifier(clf, tree_num=1, feature_names=features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.plot_tree_classifier(clf, tree_num=17, feature_names=features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis One Test Case\n",
    "### Simple Decision Model For One Test Case\n",
    "In order to locate some value different bettwen pos and neg values in one test case. That's ok to use DecisionTree\n",
    "Such as we want see more detail in `DC_IOH_VDDN_TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to get one case data\n",
    "def get_pins_by_names(df, pin_name_list):\n",
    "    all_names = []\n",
    "    for name in pin_name_list:\n",
    "        all_names += list(filter(lambda x: name in x, df.columns))\n",
    "    return all_names\n",
    "\n",
    "#pin_name_list = case2pins['DC_IOH_VDDN_TEST'][2:-3]\n",
    "pin_name_list = ['ODSPA_PGPIO_BIT0_B', 'ODSPA_PGPIO_BIT1_B',\n",
    " 'ODSPA_PCPU_INT_A', 'ODSPA_PGPIO_BIT0_A', 'ODSPA_PGPIO_BIT1_A',\n",
    " 'ODSPA_PGPIO_BIT2_B', 'ODSPA_PGPIO_BIT3_B', 'ODSPA_PGPIO_BIT4_B',\n",
    " 'ODSPA_PGPIO_BIT5_B', 'PFP_TX_B' 'ODSPA_PGPIO_BIT2_A', 'ODSPA_PGPIO_BIT3_A',\n",
    " 'ODSPA_PGPIO_BIT4_A', 'ODSPA_PGPIO_BIT5_A' ]\n",
    "print(pin_name_list)\n",
    "pin_name_list = get_pins_by_names(X_train, pin_name_list)\n",
    "X_train_onecase, X_valid_onecase = X_train[pin_name_list], X_valid[pin_name_list]\n",
    "X_train_onecase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "simple_clf = DecisionTreeClassifier(\n",
    "    max_depth=6,\n",
    "    criterion='entropy',\n",
    "    random_state=0)\n",
    "simple_clf.fit(X_train_onecase, y_train)\n",
    "\n",
    "pin_list=ph.rank_features_by_importance(simple_clf.feature_importances_, pin_name_list, max_print=10)\n",
    "graph = ph.plot_tree_classifier(simple_clf, feature_names=pin_name_list )\n",
    "graph.render('dtree_render',view=True)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test case preduce above 91 percent accuracy, it is reasonably belive these features values of test case contribute to chip failed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Predict on the test data\n",
    "predictions = simple_clf.predict(X_valid_onecase)\n",
    "print('Sample size: ',format(len(y_valid.values)))\n",
    "print('Accuracy score: ', format(accuracy_score(y_valid.values, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_valid.values, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_valid.values, predictions, average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_valid.values, predictions, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check pin value graph\n",
    "default to check top 20 imprtant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_pin(column_name):\n",
    "    df = pd.DataFrame()\n",
    "    lenth = min(pivote_df_good[column_name].size, pivote_df_bad[column_name].size)\n",
    "    df['good_' + column_name] = pivote_df_good[column_name].values[:lenth]\n",
    "    df['bad_' + column_name] = pivote_df_bad[column_name].values[:lenth]\n",
    "    df.plot(grid='on', figsize=(6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pin_name in pin_list:\n",
    "    plot_compare_pin(pin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sharp Additive Valid\n",
    "Is that value can be trust? maybe some features are same important.\n",
    "Valid entropy value by Sharp Additive Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "# limit is a target unique value count\n",
    "shap_values = explainer.shap_values(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = np.concatenate(shap_values)\n",
    "tmp2 = np.abs(tmp1)\n",
    "tmp3 = np.nanmean(tmp2,axis=0)\n",
    "print(tmp3.shape)\n",
    "np.argsort(tmp3)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gini calculate feature importance rank:')\n",
    "importance_order = ph.rank_features_by_importance(np.average(feature_importances, axis=0), features_columns, max_print=5)\n",
    "print('\\nshap additive calculate feature importance rank:')\n",
    "shap_importances = ph.rank_features_by_importance(tmp3, features_columns, max_print=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two result is different, the result given by Sharp prevails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X_valid, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check pin value by graph agin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pin_name in importance_order:\n",
    "    plot_compare_pin(pin_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
